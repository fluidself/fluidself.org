---
author: ugh
date: 2024-01-06 15:12:25+00:00
link: https://fluidself.org/books/science/how-to-create-a-mind/
slug: how-to-create-a-mind
title: 'How to Create a Mind: The Secret of Human Thought Revealed - by Ray Kurzweil'
---

**THOUGHT EXPERIMENTS ON THINKING**

Our memories are sequential and in order. They can be accessed in the order that they are remembered. We are unable to directly reverse the sequence of a memory.

There are no images, videos, or sound recordings stored in the brain. Our memories are stored as sequences of patterns. Memories that are not accessed dim over time.

We can recognize a pattern even if only part of it is perceived (seen, heard, felt) and even if it contains alterations. Our recognition ability is apparently able to detect invariant features of a pattern—characteristics that survive real-world variations.

Our conscious experience of our perceptions is actually changed by our interpretations. We are constantly predicting the future and hypothesizing what we will experience. This expectation influences what we actually perceive. Predicting the future is actually the primary reason that we have a brain.

Each of our routine procedures is remembered as an elaborate hierarchy of nested activities. The same type of hierarchy is involved in our ability to recognize objects and situations.

**A MODEL OF THE NEOCORTEX: THE PATTERN RECOGNITION THEORY OF MIND**

The neocortex is responsible for sensory perception, recognition of everything from visual objects to abstract concepts, controlling movement, reasoning from spatial orientation to rational thought, and language—basically, what we regard as “thinking.”

Human beings have only a weak ability to process logic, but a very deep core capability of recognizing patterns. To do logical thinking, we need to use the neocortex, which is basically a large pattern recognizer. It is not an ideal mechanism for performing logical transformations, but it is the only facility we have for the job.

The pattern recognition theory of mind that I present here is based on the recognition of patterns by pattern recognition modules in the neocortex. These patterns (and the modules) are organized in hierarchies.

An important attribute of the PRTM is how the recognitions are made inside each pattern recognition module. Stored in the module is a weight for each input dendrite indicating how important that input is to the recognition. The pattern recognizer has a threshold for firing (which indicates that this pattern recognizer has successfully recognized the pattern it is responsible for). Not every input pattern has to be present for a recognizer to fire. The recognizer may still fire if an input with a low weight is missing, but it is less likely to fire if a high-importance input is missing. When it fires, a pattern recognizer is basically saying, “The pattern I am responsible for is probably present.”

A very important point to note here is that information flows down the conceptual hierarchy as well as up. If anything, this downward flow is even more significant.

The neocortex is, therefore, predicting what it expects to encounter. Envisaging the future is one of the primary reasons we have a neocortex. At the highest conceptual level, we are continually making predictions—who is going to walk through the door next, what someone is likely to say next, what we expect to see when we turn the corner, the likely results of our own actions, and so on. These predictions are constantly occurring at every level of the neocortex hierarchy. We often misrecognize people and things and words because our threshold for confirming an expected pattern is too low.

The input to each pattern processor is a one-dimensional list, even though the pattern itself may inherently reflect more than one dimension.

We should factor in at this point the insight that the patterns we have learned to recognize (for example, a specific dog or the general idea of a “dog,” a musical note or a piece of music) are exactly the same mechanism that is the basis for our memories. Our memories are in fact patterns organized as lists (where each item in each list is another pattern in the cortical hierarchy) that we have learned and then recognize when presented with the appropriate stimulus. In fact, memories exist in the neocortex in order to be recognized.

Each pattern in our neocortex is meaningful only in light of all the information carried in the levels below it. Moreover, other patterns at the same level and at higher levels are also relevant in interpreting a particular pattern because they provide context. True mind reading, therefore, would necessitate not just detecting the activations of the relevant axons in a person’s brain, but examining essentially her entire neocortex with all of its memories to understand these activations.

As we experience our own thoughts and memories, we “know” what they mean, but they do not exist as readily explainable thoughts and recollections. If we want to share them with others, we need to translate them into language. This task is also accomplished by the neocortex, using pattern recognizers trained with patterns that we have learned for the purpose of using language. Language is itself highly hierarchical and evolved to take advantage of the hierarchical nature of the neocortex, which in turn reflects the hierarchical nature of reality. The innate ability of humans to learn the hierarchical structures in language that Noam Chomsky wrote about reflects the structure of the neocortex.

We have examined how we recognize (sensory and perceptual) patterns and recall sequences of patterns (our memory of things, people, and events). However, we are not born with a neocortex filled with any of these patterns. Our neocortex is virgin territory when our brain is created. It has the capability of learning and therefore of creating connections between its pattern recognizers, but it gains those connections from experience.

Learning and recognition take place simultaneously. We start learning immediately, and as soon as we’ve learned a pattern, we immediately start recognizing it. The neocortex is continually trying to make sense of the input presented to it. If a particular level is unable to fully process and recognize a pattern, it gets sent to the next higher level. If none of the levels succeeds in recognizing a pattern, it is deemed to be a new pattern. Classifying a pattern as new does not necessarily mean that every aspect of it is new.

One important point that applies to both our biological neocortex and attempts to emulate it is that it is difficult to learn too many conceptual levels simultaneously. We can essentially learn one or at most two conceptual levels at a time. Once that learning is relatively stable, we can go on to learn the next level. We may continue to fine-tune the learning in the lower levels, but our learning focus is on the next level of abstraction. This is true at both the beginning of life, as newborns struggle with basic shapes, and later in life, as we struggle to learn new subject matter, one level of complexity at a time. We find the same phenomenon in machine emulations of the neocortex. However, if they are presented increasingly abstract material one level at a time, machines are capable of learning just as humans do (although not yet with as many conceptual levels).

**THE BIOLOGICAL NEOCORTEX**

Once biological evolution stumbled on a neural mechanism capable of hierarchical learning, it found it to be immensely useful for evolution’s one objective, which is survival. The benefit of having a neocortex became acute when quickly changing circumstances favored rapid learning. Species of all kinds—plants and animals—can learn to adapt to changing circumstances over time, but without a neocortex they must use the process of genetic evolution. It can take a great many generations—thousands of years—for a species without a neocortex to learn significant new behaviors (or in the case of plants, other adaptation strategies). The salient survival advantage of the neocortex was that it could learn in a matter of days. If a species encounters dramatically changed circumstances and one member of that species invents or discovers or just stumbles upon (these three methods all being variations of innovation) a way to adapt to that change, other individuals will notice, learn, and copy that method, and it will quickly spread virally to the entire population.

The basic unit of the neocortex is a module of neurons, which I estimate at around a hundred. These are woven together into each neocortical column so that each module is not visibly distinct. The pattern of connections and synaptic strengths within each module is relatively stable. It is the connections and synaptic strengths between modules that represent learning.

Signals go up and down the conceptual hierarchy. A signal going up means, “I’ve detected a pattern.” A signal going down means, “I’m expecting your pattern to occur,” and is essentially a prediction. Both upward and downward signals can be either excitatory or inhibitory.

There is one more piece of corroborating evidence. The techniques that we have evolved over the past several decades in the field of artificial intelligence to recognize and intelligently process real-world phenomena (such as human speech and written language) and to understand natural-language documents turn out to be mathematically similar to the model I have presented above. They are also examples of the PRTM. The AI field was not explicitly trying to copy the brain, but it nonetheless arrived at essentially equivalent techniques.

**THE OLD BRAIN**

Our old brain—the one we had before we were mammals—has not disappeared. Indeed it still provides much of our motivation in seeking gratification and avoiding danger. These goals are modulated, however, by our neocortex, which dominates the human brain in both mass and activity.

If the neocortex is good at solving problems, then what is the main problem we are trying to solve? The problem that evolution has always tried to solve is survival of the species. That translates into the survival of the individual, and each of us uses his or her own neocortex to interpret that in myriad ways. In order to survive, animals need to procure their next meal while at the same time avoiding becoming someone else’s meal. They also need to reproduce. The earliest brains evolved pleasure and fear systems that rewarded the fulfillment of these fundamental needs along with basic behaviors that facilitated them. As environments and competing species gradually changed, biological evolution made corresponding alterations. With the advent of hierarchical thinking, the satisfaction of critical drives became more complex, as it was now subject to the vast complex of ideas within ideas. But despite its considerable modulation by the neocortex, the old brain is still alive and well and still motivating us with pleasure and fear.

It is fair to say that our emotional experiences take place in both the old and the new brains. Thinking takes place in the new brain (the neocortex), but feeling takes place in both. Any emulation of human behavior will therefore need to model both. However, if it is just human cognitive intelligence that we are after, the neocortex is sufficient. We can replace the old brain with the more direct motivation of a nonbiological neocortex to achieve the goals that we assign to it.

**TRANSCENDENT ABILITIES**

Our emotional thoughts also take place in the neocortex but are influenced by portions of the brain ranging from ancient brain regions such as the amygdala to some evolutionarily recent brain structures such as the spindle neurons, which appear to play a key role in higher-level emotions. Unlike the regular and logical recursive structures found in the cerebral cortex, the spindle neurons have highly irregular shapes and connections. They are the largest neurons in the human brain, spanning its entire breadth. They are deeply interconnected, with hundreds of thousands of connections tying together diverse portions of the neocortex.

One way to achieve greater creativity is by effectively assembling more neocortex. One approach to expand the available neocortex is through the collaboration of multiple humans. This is accomplished routinely via the communication between people gathered in a problem-solving community.

The next step, of course, will be to expand the neocortex itself with its nonbiological equivalent. This will be our ultimate act of creativity: to create the capability of being creative. A nonbiological neocortex will ultimately be faster and could rapidly search for the kinds of metaphors that inspired Darwin and Einstein. It could systematically explore all of the overlapping boundaries between our exponentially expanding frontiers of knowledge.

Whether we access such expanded intelligence through direct neural connection or the way we do now—by interacting with it via our devices—is an arbitrary distinction. In my view we will all become more creative through this pervasive enhancement, whether we choose to opt in or out of direct connection to humanity’s expanded intelligence. We have already outsourced much of our personal, social, historical, and cultural memory to the cloud, and we will ultimately do the same thing with our hierarchical thinking.

From an evolutionary perspective, love itself exists to meet the needs of the neocortex. If we didn’t have a neocortex, then lust would be quite sufficient to guarantee reproduction. The ecstatic instigation of love leads to attachment and mature love, and results in a lasting bond. This in turn is designed to provide at least the possibility of a stable environment for children while their own neocortices undergo the critical learning needed to become responsible and capable adults. Learning in a rich environment is inherently part of the method of the neocortex.

**THE BIOLOGICALLY INSPIRED DIGITAL NEOCORTEX**

To appreciate the significance of the evolution of the neocortex, consider that it greatly sped up the process of learning (hierarchical knowledge) from thousands of years to months (or less). Even if millions of animals in a particular mammalian species failed to solve a problem (requiring a hierarchy of steps), it required only one to accidentally stumble upon a solution. That new method would then be copied and spread exponentially through the population. We are now in a position to speed up the learning process by a factor of thousands or millions once again by migrating from biological to nonbiological intelligence. Once a digital neocortex learns a skill, it can transfer that know-how in minutes or even seconds.

When we augment our own neocortex with a synthetic version, we won’t have to worry about how much additional neocortex can physically fit into our bodies and brains, as most of it will be in the cloud, like most of the computing we use today. I estimated earlier that we have on the order of 300 million pattern recognizers in our biological neocortex. That’s as much as could be squeezed into our skulls even with the evolutionary innovation of a large forehead and with the neocortex taking about 80 percent of the available space. As soon as we start thinking in the cloud, there will be no natural limits—we will be able to use billions or trillions of pattern recognizers, basically whatever we need, and whatever the law of accelerating returns can provide at each point in time. In order for a digital neocortex to learn a new skill, it will still require many iterations of education, just as a biological neocortex does, but once a single digital neocortex somewhere and at some time learns something, it can share that knowledge with every other digital neocortex without delay. We can each have our own private neocortex extenders in the cloud, just as we have our own private stores of personal data today. Last but not least, we will be able to back up the digital portion of our intelligence. As we have seen, it is not just a metaphor to state that there is information contained in our neocortex, and it is frightening to contemplate that none of this information is backed up today. There is, of course, one way in which we do back up some of the information in our brains—by writing it down. The ability to transfer at least some of our thinking to a medium that can outlast our biological bodies was a huge step forward, but a great deal of data in our brains continues to remain vulnerable.

Since the neural net wiring and synaptic weights are initially set randomly, the answers of an untrained neural net are also random. The key to a neural net, therefore, is that it must learn its subject matter, just like the mammalian brains on which it’s supposedly modeled. A neural net starts out ignorant; its teacher—which may be a human, a computer program, or perhaps another, more mature neural net that has already learned its lessons—rewards the student neural net when it generates the correct output and punishes it when it does not. This feedback is in turn used by the student neural net to adjust the strength of each interneuronal connection. Connections that are consistent with the correct answer are made stronger. Those that advocate a wrong answer are weakened. Over time the neural net organizes itself to provide the correct answers without coaching. Experiments have shown that neural nets can learn their subject matter even with unreliable teachers. If the teacher is correct only 60 percent of the time, the student neural net will still learn its lessons with an accuracy approaching 100 percent.

It is amusing and ironic when observers criticize Watson for just doing statistical analysis of language as opposed to possessing the “true” understanding of language that humans have. Hierarchical statistical analysis is exactly what the human brain is doing when it is resolving multiple hypotheses based on statistical inference (and indeed at every level of the neocortical hierarchy). Both Watson and the human brain learn and respond based on a similar approach to hierarchical understanding. In many respects Watson’s knowledge is far more extensive than a human’s; no human can claim to have mastered all of Wikipedia, which is only part of Watson’s knowledge base. Conversely, a human can today master more conceptual levels than Watson, but that is certainly not a permanent gap.

It is my view that self-organizing methods such as I articulated in the pattern recognition theory of mind are needed to understand the elaborate and often ambiguous hierarchies we encounter in real-world phenomena, including human language. An ideal combination for a robustly intelligent system would be to combine hierarchical intelligence based on the PRTM (which I contend is how the human brain works) with precise codification of scientific knowledge and data. That essentially describes a human with a computer. We will enhance both poles of intelligence in the years ahead. With regard to our biological intelligence, although our neocortex has significant plasticity, its basic architecture is limited by its physical constraints. Putting additional neocortex into our foreheads was an important evolutionary innovation, but we cannot now easily expand the size of our frontal lobes by a factor of a thousand, or even by 10 percent. That is, we cannot do so biologically, but that is exactly what we will do technologically.

**THE MIND AS COMPUTER**

The issue of whether or not the computer and the human brain are at some level equivalent remains controversial today. In the introduction I mentioned that there were millions of links for quotations on the complexity of the human brain. Similarly, a Google inquiry for “Quotations: the brain is not a computer” also returns millions of links. In my view, statements along these lines are akin to saying, “Applesauce is not an apple.” Technically that statement is true, but you can make applesauce from an apple. Perhaps more to the point, it is like saying, “Computers are not word processors.” It is true that a computer and a word processor exist at different conceptual levels, but a computer can become a word processor if it is running word processing software and not otherwise. Similarly, a computer can become a brain if it is running brain software.

The universality of computation (the concept that a general-purpose computer can implement any algorithm)—and the power of this idea—emerged at the same time as the first actual machines. There are four key concepts that underlie the universality and feasibility of computation and its applicability to our thinking. They are worth reviewing here, because the brain itself makes use of them. The first is the ability to communicate, remember, and compute information reliably.

The second important idea on which the information age relies is the one I mentioned earlier: the universality of computation. Even though the Turing machine has only a handful of commands and processes only one bit at a time, it can compute anything that any computer can compute. Another way to say this is that any machine that is “Turing complete” (that is, that has equivalent capabilities to a Turing machine) can compute any algorithm (any procedure that we can define).

John von Neumann created the architecture of the modern computer, which represents our third major idea. The von Neumann model includes a central processing unit, where arithmetical and logical operations are carried out; a memory unit, where the program and data are stored; mass storage; a program counter; and input/output channels.

That brings us to the fourth important idea, which is to go beyond Ada Byron’s conclusion that a computer could not think creatively and find the key algorithms employed by the brain and then use these to turn a computer into a brain. Even though the architecture and building blocks appear to be radically different between brain and computer, we can nonetheless conclude that a von Neumann machine can simulate the processing in a brain. The converse does not hold, however, because the brain is not a von Neumann machine and does not have a stored program as such (albeit we can simulate a very simple Turing machine in our heads). Its algorithm or methods are implicit in its structure.

The speed of neural processing is extremely slow, on the order of a hundred calculations per second, but the brain compensates for this through massive parallel processing.

Von Neumann’s fundamental insight was that there is an essential equivalence between a computer and the brain. Note that the emotional intelligence of a biological human is part of its intelligence. If von Neumann’s insight is correct, and if one accepts my own leap of faith that a nonbiological entity that convincingly re-creates the intelligence (emotional and otherwise) of a biological human is conscious, then one would have to conclude that there is an essential equivalence between a computer—with the right software—and a (conscious) mind.

**THOUGHT EXPERIMENTS ON THE MIND**

Where consciousness is concerned, the guiding principle is “you gotta have faith”—that is, we each need a leap of faith as to what and who is conscious, and who and what we are as conscious beings. Otherwise we could not get up in the morning. But we should be honest about the fundamental need for a leap of faith in this matter and self-reflective as to what our own particular leap involves. People have very different leaps, despite impressions to the contrary. Individual philosophical assumptions about the nature and source of consciousness underlie disagreements on issues ranging from animal rights to abortion, and will result in even more contentious future conflicts over machine rights. My objective prediction is that machines in the future will appear to be conscious and that they will be convincing to biological people when they speak of their qualia. They will exhibit the full range of subtle, familiar emotional cues; they will make us laugh and cry; and they will get mad at us if we say that we don’t believe that they are conscious. (They will be very smart, so we won’t want that to happen.) We will come to accept that they are conscious persons. My own leap of faith is this: Once machines do succeed in being convincing when they speak of their qualia and conscious experiences, they will indeed constitute conscious persons. I have come to my position via this thought experiment: Imagine that you meet an entity in the future (a robot or an avatar) that is completely convincing in her emotional reactions. She laughs convincingly at your jokes, and in turn makes you laugh and cry (but not just by pinching you). She convinces you of her sincerity when she speaks of her fears and longings. In every way, she seems conscious. She seems, in fact, like a person. Would you accept her as a conscious person? If your initial reaction is that you would likely detect some way in which she betrays her nonbiological nature, then you are not keeping to the assumptions in this hypothetical situation, which established that she is fully convincing. Given that assumption, if she were threatened with destruction and responded, as a human would, with terror, would you react in the same empathetic way that you would if you witnessed such a scene involving a human? For myself, the answer is yes, and I believe the answer would be the same for most if not virtually all other people regardless of what they might assert now in a philosophical debate. Again, the emphasis here is on the word “convincing.”

There is certainly disagreement on when or even whether we will encounter such a nonbiological entity. My own consistent prediction is that this will first take place in 2029 and become routine in the 2030s. But putting the time frame aside, I believe that we will eventually come to regard such entities as conscious.

If you do accept the leap of faith that a nonbiological entity that is convincing in its reactions to qualia is actually conscious, then consider what that implies: namely that consciousness is an emergent property of the overall pattern of an entity, not the substrate it runs on. There is a conceptual gap between science, which stands for objective measurement and the conclusions we can draw thereby, and consciousness, which is a synonym for subjective experience. We obviously cannot simply ask an entity in question, “Are you conscious?” If we look inside its “head,” biological or otherwise, to ascertain that, then we would have to make philosophical assumptions in determining what it is that we are looking for. The question as to whether or not an entity is conscious is therefore not a scientific one. Based on this, some observers go on to question whether consciousness itself has any basis in reality.

Because a great deal of our moral and legal system is based on protecting the existence of and preventing the unnecessary suffering of conscious entities, in order to make responsible judgments we need to answer the question as to who is conscious. That question is therefore not simply a matter for intellectual debate, as is evident in the controversy surrounding an issue like abortion. I should point out that the abortion issue can go somewhat beyond the issue of consciousness, as pro-life proponents argue that the potential for an embryo to ultimately become a conscious person is sufficient reason for it to be awarded protection, just as someone in a coma deserves that right. But fundamentally the issue is a debate about when a fetus becomes conscious.

There are two ways to view the questions we have been considering—converse Western and Eastern perspectives on the nature of consciousness and of reality. In the Western perspective, we start with a physical world that evolves patterns of information. After a few billion years of evolution, the entities in that world have evolved sufficiently to become conscious beings. In the Eastern view, consciousness is the fundamental reality; the physical world only comes into existence through the thoughts of conscious beings. The physical world, in other words, is the thoughts of conscious beings made manifest. These are of course simplifications of complex and diverse philosophies, but they represent the principal polarities in the philosophies of consciousness and its relationship to the physical world.

This is how I resolve the Western-Eastern divide on consciousness and the physical world. In my view, both perspectives have to be true. On the one hand, it is foolish to deny the physical world. Even if we do live in a simulation, as speculated by Swedish philosopher Nick Bostrom, reality is nonetheless a conceptual level that is real for us. If we accept the existence of the physical world and the evolution that has taken place in it, then we can see that conscious entities have evolved from it. On the other hand, the Eastern perspective—that consciousness is fundamental and represents the only reality that is truly important—is also difficult to deny. Just consider the precious regard we give to conscious persons versus unconscious things. We consider the latter to have no intrinsic value except to the extent that they can influence the subjective experience of conscious persons. Even if we regard consciousness as an emergent property of a complex system, we cannot take the position that it is just another attribute (along with “digestion” and “lactation,” to quote John Searle). It represents what is truly important.

The word “spiritual” is often used to denote the things that are of ultimate significance. Many people don’t like to use such terminology from spiritual or religious traditions, because it implies sets of beliefs that they may not subscribe to. But if we strip away the mystical complexities of religious traditions and simply respect “spiritual” as implying something of profound meaning to humans, then the concept of consciousness fits the bill. It reflects the ultimate spiritual value. Indeed, “spirit” itself is often used to denote consciousness. Evolution can then be viewed as a spiritual process in that it creates spiritual beings, that is, entities that are conscious. Evolution also moves toward greater complexity, greater knowledge, greater intelligence, greater beauty, greater creativity, and the ability to express more transcendent emotions, such as love. These are all descriptions that people have used for the concept of God, albeit God is described as having no limitations in these regards.

People often feel threatened by discussions that imply the possibility that a machine could be conscious, as they view considerations along these lines as a denigration of the spiritual value of conscious persons. But this reaction reflects a misunderstanding of the concept of a machine. Such critics are addressing the issue based on the machines they know today, and as impressive as they are becoming, I agree that contemporary examples of technology are not yet worthy of our respect as conscious beings. My prediction is that they will become indistinguishable from biological humans, whom we do regard as conscious beings, and will therefore share in the spiritual value we ascribe to consciousness. This is not a disparagement of people; rather, it is an elevation of our understanding of (some) future machines. We should probably adopt a different terminology for these entities, as they will be a different sort of machine. Indeed, as we now look inside the brain and decode its mechanisms we discover methods and algorithms that we can not only understand but re-create—“the parts of a mill pushing on each other,” to paraphrase German mathematician and philosopher Gottfried Wilhelm Leibniz (1646–1716) when he wrote about the brain. Humans already constitute spiritual machines. Moreover, we will merge with the tools we are creating so closely that the distinction between human and machine will blur until the difference disappears. That process is already well under way, even if most of the machines that extend us are not yet inside our bodies and brains.

Since there is a reasonable consensus among philosophers that free will does imply conscious decision making, it appears to be one prerequisite for free will. However, to many observers, consciousness is a necessary but not sufficient condition. If our decisions—conscious or otherwise—are predetermined before we make them, how can we say that our decisions are free? This position, which holds that free will and determinism are not compatible, is known as incompatibilism.

Not everyone regards determinism as being incompatible with the concept of free will, however. The compatibilists argue, essentially, that you’re free to decide what you want even though what you decide is or may be determined. Daniel Dennett, for example, argues that while the future may be determined from the state of the present, the reality is that the world is so intricately complex that we cannot possibly know what the future will bring. We can identify what he refers to as “expectations,” and we are indeed free to perform acts that differ from these expectations. We should consider how our decisions and actions compare to these expectations, not to a theoretically determined future that we cannot in fact know. That, Dennett argues, is sufficient for free will.

The concept of free will—and responsibility, which is a closely aligned idea—is useful, and indeed vital, to maintaining social order, whether or not free will actually exists. Just as consciousness clearly exists as a meme, so too does free will. Attempts to prove its existence, or even to define it, may become hopelessly circular, but the reality is that almost everyone believes in the idea. Very substantial portions of our higher-level neocortex are devoted to the concept that we make free choices and are responsible for our actions. Whether in a strict philosophical sense that is true or even possible, society would be far worse off if we did not have such beliefs.

Furthermore, the world is not necessarily determined. Dr. Wolfram’s primary thesis is that the world is one big class IV cellular automaton. The reason that his book is titled A New Kind of Science is because this theory contrasts with most other scientific laws. If there is a satellite orbiting Earth, we can predict where it will be five years from now without having to run through each moment of a simulated process by using the relevant laws of gravity and solve where it will be at points in time far in the future. But the future state of class IV cellular automata cannot be predicted without simulating every step along the way. If the universe is a giant cellular automaton, as Dr. Wolfram postulates, there would be no computer big enough—since every computer would be a subset of the universe—that could run such a simulation. Therefore the future state of the universe is completely unknowable even though it is deterministic. Thus even though our decisions are determined (because our bodies and brains are part of a deterministic universe), they are nonetheless inherently unpredictable because we live in (and are part of) a class IV automaton. We cannot predict the future of a class IV automaton except to let the future unfold. For Dr. Wolfram, this is sufficient to allow for free will.

My own leap of faith is that I believe that humans have free will, and while I act as if that is the case, I am hard pressed to find examples among my own decisions that illustrate that. Consider the decision to write this book—I never made that decision. Rather, the idea of the book decided that for me. In general, I find myself captive to ideas that seem to implant themselves in my neocortex and take over. How about the decision to get married, which I made (in collaboration with one other person) thirty-six years ago? At the time, I had been following the usual program of being attracted to—and pursuing—a pretty girl. I then fell in love. Where is the free will in that?

Although I share Descartes’ confidence that I am conscious, I’m not so sure about free will. It is difficult to escape Schopenhauer’s conclusion that “you can do what you will, but in any given moment of your life you can will only one definite thing and absolutely nothing other than that one thing.” Nonetheless I will continue to act as if I have free will and to believe in it, so long as I don’t have to explain why.

What I wonder about ever more than whether or not I am conscious or exercise free will is why I happen to be conscious of the experiences and decisions of this one particular person who writes books, enjoys hiking and biking, takes nutritional supplements, and so on. An obvious answer would be, “Because that’s who you are.” That exchange is probably no more tautological than my answers above to questions about consciousness and free will. But actually I do have a better answer for why my consciousness is associated with this particular person: It is because that is who I created myself to be. A common aphorism is, “You are what you eat.” It is even more true to say, “You are what you think.” As we have discussed, all of the hierarchical structures in my neocortex that define my personality, skills, and knowledge are the result of my own thoughts and experiences. The people I choose to interact with and the ideas and projects I choose to engage in are all primary determinants of who I become. For that matter, what I eat also reflects the decisions made by my neocortex. Accepting the positive side of the free will duality for the moment, it is my own decisions that result in who I am.

Are you the same person you were a few months ago? Certainly there are some differences. Perhaps you learned a few things. But you assume that your identity persists, that you are not continually destroyed and re-created. Consider a river, like the one that flows past my office. As I look out now at what people call the Charles River, is it the same river that I saw yesterday? Let’s first reflect on what a river is. The dictionary defines it is “a large natural stream of flowing water.” By that definition, the river I’m looking at is a completely different one than it was yesterday. Every one of its water molecules has changed, a process that happens very quickly. Greek philosopher Diogenes Laertius wrote in the third century AD that “you cannot step into the same river twice.” But that is not how we generally regard rivers. People like to look at them because they are symbols of continuity and stability. By the common view, the Charles River that I looked at yesterday is the same river I see today. Our lives are much the same. Fundamentally we are not the stuff that makes up our bodies and brains. These particles essentially flow through us in the same way that water molecules flow through a river. We are a pattern that changes slowly but has stability and continuity, even though the stuff constituting the pattern changes quickly. The gradual introduction of nonbiological systems into our bodies and brains will be just another example of the continual turnover of parts that compose us. It will not alter the continuity of our identity any more than the natural replacement of our biological cells does. We have already largely outsourced our historical, intellectual, social, and personal memories to our devices and the cloud. The devices we interact with to access these memories may not yet be inside our bodies and brains, but as they become smaller and smaller (and we are shrinking technology at a rate of about a hundred in 3-D volume per decade), they will make their way there. In any event, it will be a useful place to put them—we won’t lose them that way. If people do opt out of placing microscopic devices inside their bodies, that will be fine, as there will be other ways to access the pervasive cloud intelligence.

What I believe will actually happen is that we will continue on the path of the gradual replacement and augmentation scenario until ultimately most of our thinking will be in the cloud. My leap of faith on identity is that identity is preserved through continuity of the pattern of information that makes us us. Continuity does allow for continual change, so whereas I am somewhat different than I was yesterday, I nonetheless have the same identity. However, the continuity of the pattern that constitutes my identity is not substrate-dependent. Biological substrates are wonderful—they have gotten us very far—but we are creating a more capable and durable substrate for very good reasons.

**OBJECTIONS**

My core thesis, which I call the law of accelerating returns (LOAR), is that fundamental measures of information technology follow predictable and exponential trajectories, belying the conventional wisdom that “you can’t predict the future.” There are still many things—which project, company, or technical standard will prevail in the marketplace, when peace will come to the Middle East—that remain unknowable, but the underlying price/performance and capacity of information has nonetheless proven to be remarkably predictable. Surprisingly, these trends are unperturbed by conditions such as war or peace and prosperity or recession.

The most significant source of objection to my thesis on the law of accelerating returns and its application to the amplification of human intelligence stems from the linear nature of human intuition. Each of the several hundred million pattern recognizers in the neocortex processes information sequentially. One of the implications of this organization is that we have linear expectations about the future, so critics apply their linear intuition to information phenomena that are fundamentally exponential. I call objections along these lines “criticism from incredulity,” in that exponential projections seem incredible given our linear predilection, and they take a variety of forms.

The way that the massively redundant structures in the brain differentiate is through learning and experience. The current state of the art in AI does in fact enable systems to also learn from their own experience. The Google self-driving cars learn from their own driving experience as well as from data from Google cars driven by human drivers; Watson learned most of its knowledge by reading on its own. It is interesting to note that the methods deployed today in AI have evolved to be mathematically very similar to the mechanisms in the neocortex.

It appears to me that many critics will not be satisfied until computers routinely pass the Turing test, but even that threshold will not be clear-cut. Undoubtedly, there will be controversy as to whether claimed Turing tests that have been administered are valid. Indeed, I will probably be among those critics disparaging early claims along these lines. By the time the arguments about the validity of a computer passing the Turing test do settle down, computers will have long since surpassed unenhanced human intelligence. My emphasis here is on the word “unenhanced,” because enhancement is precisely the reason that we are creating these “mind children,” as Hans Moravec calls them. Combining human-level pattern recognition with the inherent speed and accuracy of computers will result in very powerful abilities. But this is not an alien invasion of intelligent machines from Mars—we are creating these tools to make ourselves smarter. I believe that most observers will agree with me that this is what is unique about the human species: We build these tools to extend our own reach.

**EPILOGUE**

British mathematician Irvin J. Good, a colleague of Alan Turing’s, wrote in 1965 that “the first ultraintelligent machine is the last invention that man need ever make.” He defined such a machine as one that could surpass the “intellectual activities of any man however clever” and concluded that “since the design of machines is one of these intellectual activities, an ultraintelligent machine could design even better machines; there would then unquestionably be an ‘intelligence explosion.’” The last invention that biological evolution needed to make—the neocortex—is inevitably leading to the last invention that humanity needs to make—truly intelligent machines—and the design of one is inspiring the other. Biological evolution is continuing but technological evolution is moving a million times faster than the former. According to the law of accelerating returns, by the end of this century we will be able to create computation at the limits of what is possible, based on the laws of physics as applied to computation. We call matter and energy organized in this way “computronium,” which is vastly more powerful pound per pound than the human brain. It will not just be raw computation but will be infused with intelligent algorithms constituting all of human-machine knowledge. Over time we will convert much of the mass and energy in our tiny corner of the galaxy that is suitable for this purpose to computronium. Then, to keep the law of accelerating returns going, we will need to spread out to the rest of the galaxy and universe.

Cosmologists argue about whether the world will end in fire (a big crunch to match the big bang) or ice (the death of the stars as they spread out into an eternal expansion), but this does not take into account the power of intelligence, as if its emergence were just an entertaining sideshow to the grand celestial mechanics that now rule the universe. How long will it take for us to spread our intelligence in its nonbiological form throughout the universe? If we can transcend the speed of light—admittedly a big if—for example, by using wormholes through space (which are consistent with our current understanding of physics), it could be achieved within a few centuries. Otherwise, it will likely take much longer. In either scenario, waking up the universe, and then intelligently deciding its fate by infusing it with our human intelligence in its nonbiological form, is our destiny.
