---
date: 2024-04-23 15:02:25+00:00
link: https://fluidself.org/books/science/how-life-works
slug: how-life-works
title: 'How Life Works: A User’s Guide to the New Biology - by Philip Ball'
---

When organisms first became multicellular, when they became able to adjust to and exploit the full richness of their surroundings through sensory modalities like vision and smell, when their sensitivity and receptivity to the environment became genuine cognition, it seems that life increasingly relinquished a strategy of prescribing the response of the organism to every stimulus, and instead supplied the basic ingredients for systems that could devise and improvise solutions to living that are emergent, versatile, adaptive, and robust. The new picture dispels the long-standing idea that living systems must be regarded as machines.

The problem of defining “life” has bedeviled biology throughout its history, and still there is no agreed resolution. But one of the best ways to characterize living entities is not through any of the features or properties usually considered to define it, such as replication, metabolism, or evolution. Rather, living entities are generators of meaning. They mine their environment (including their own bodies) for things that have meaning for them: moisture, nutrients, warmth. It is not sentimental but simply following the same logic to say that, for we human organisms, another of those meaningful things is love.

### The End of the Machine: A New View of Life

One of the fundamental messages of this book is that we cannot properly understand how life works through analogies or metaphorical comparison with any technology that humans have ever invented (so far). Such analogies may provide a foothold for our understanding, but in the end they will fall short, and will constrain and even mislead us if we don’t recognize when to relinquish them.

Only for living things—or, to speak more generally, for things that, by their very nature, are imbued with purposes and goals—can there be a “point.” I suspect it is in fact precisely by virtue of being a thing that has autonomous goals, and that can autonomously attribute meaning, that an entity can be said to be alive. For some feature of an organism’s environment to acquire meaning, the organism doesn’t have to be “aware” of it. I don’t think (although some biologists would dispute this) that a bacterium is conscious of its environment. The organism simply needs mechanisms for evaluating the value of that feature and acting accordingly. Looked at this way, life can be considered to be a meaning generator. Living things are, you could say, those entities capable of attributing value in their environment, and thereby finding a point to the universe.

### Genes: What DNA Really Does

Living things are not, after all, complex yet deterministic readouts of genes. In Michel Morange’s words, “Biological processes are genetically controlled, but this does not imply that the gene products are in and of themselves responsible for these processes. They are simply components that participate in these processes.”

The gene’s-eye view of life (indeed, even of evolution) is shaped by a particular scientific model and is valid only within the context of that model. It does not and cannot deliver an account of the world as we find it. The problem with atomizing organisms into genes is that genes are not alive—and once you have set aside life to get to the gene, you can’t get it back again. The gene is far too atomized a unit to tell us much at all about how life works.

Not only has our picture of what genes are and what they do, what DNA and RNA and proteins are like and how they interact, become far more complex than the simple information flow envisaged by the early pioneers of molecular biology, but within those very changes of thinking lie the seeds of a substantially different view of life.

The fundamental mechanism of information storage and transfer in nucleic acids—complementary base pairing—is so elegant that it risks blinding us to the awesome sophistication of the full process of getting that information in and out, let alone what it actually means. The molecules do not simply wander up to one another and start talking. You might say that they must first be assigned that task, and must then petition higher levels of organization before permission is granted. For gene transcription is carefully _regulated_. For example, genes on DNA are accompanied by sequences near (and “upstream” of, in terms of the direction of readout) their start, typically a hundred to a thousand base pairs long, called _promoter_ sites. Particular proteins must bind to these regions in order for transcription by RNA polymerase to begin. Several different molecules, especially proteins called transcription factors, are involved in this initiation process. What happens at a promoter site determines whether transcription _can_ happen; but the chance that it _will_ is influenced by DNA sequences called _enhancers_, of which there are hundreds of thousands in the human genome, typically fifty to fifteen hundred base pairs long.

At any rate, whatever it is that a genome encodes, this is nothing like the way information is encoded on magnetic tape or in a sequence of algorithmic instructions. What it perhaps better resembles is the organization of the brain. There was a time when the brain too was thought to be neatly compartmentalized into sections with specific roles in behavior: this region controls friendliness, that region is responsible for aggression or perseverance. Such a picture provided the basis of the nineteenth century discipline of phrenology, according to which it was believed that measurements of the skull could reveal the disposition and shape of the brain beneath it and thus disclose aspects of character.

The old view of genes as distinct segments of DNA strung along the chromosomes like beads, interspersed with junk, and each controlling some aspect of phenotype, was basically a kind of genetic phrenology. That’s not to ridicule it, for after all, even the central idea in phrenology—that the brain is composed of modules—is not entirely wrong. There _are_ regions of the brain that have specialized functions: one region is implicated in language processing, another in vision, another in spatial memory, and so on. Damage confined to one of these areas can result in extremely specific cognitive impairments without affecting other abilities. But to suppose that the independent and autonomous operation of each region produces specific aspects of behavior is wrong. The brain relies on complex interactions between them, and has to integrate the activity of each area into a coherent response in a way that we still don’t fully understand. And so it is for the genome too.

Perhaps in this respect, then, David Baltimore was right to call the genome the “brain of a cell” in 1984. But in a deeper sense he was quite wrong, for his implication was that the genome _controls_ the cell. It does not. Rather, it supplies resources for the cell as an autonomous and integrated entity. We call the origin of those resources genes—but in truth genes are not so much parts of the DNA molecule as conceptually derived from them. So genes are no more a blueprint for our bodies than they are a blueprint for our minds, or indeed our lives. They impart capabilities; the rest is up to us, in interaction with our environment.

We now see that most of the genes in the human genome do not have any assigned function; that is, we can’t really say what they “do.” And this isn’t just a matter of getting round to figuring that out. It seems likely that for many of them—especially those most central to the way our cells and bodies work, like _Wnt_—it is meaningless to try to assign a function of this sort. All we might be able to say about such a gene’s “function” is that, as Crick said, it encodes a protein, or, often, a family of related proteins. Some genes and active regions of the genome don’t even do that. In the story of how life works, the role of genes, or more generally of the genome, seems then to be limited to saying that they carry chemically encoded information used for making molecules with biological functions, and that this information can be inherited. That’s it.

Is the genome _informationally complete_ to specify an organism (as we surely might expect of an instruction book or a blueprint)? The answer to that is an unequivocal “No.” You can’t compute from the genome how an organism will turn out, not even in principle. There is plenty that happens during development that is not hardwired by genes. And from a single protein-coding gene, you can’t even tell in general what the product of its expression will be, let alone what function that product will serve in the cell. You couldn’t even predict that a genome _will_ build a cell—for the simple reason that a genome _can’t_ build a cell. Rather, the cell is a precondition for anything the genome _can_ do.

### RNA and Transcription: Reading the Message

Why do things in this roundabout way? Why didn’t evolution find enzymes that can make proteins directly from the DNA template? No one knows the full answer, but it is surely in part that _this is not the objective_. Genes—or more generally, functional sections of DNA—are _not_ simply instructions for making proteins. Rather, some of them determine what kinds of proteins cells, tissues and organs can make. Some do other things: RNA-related things that don’t directly involve proteins at all. Another answer is that making a protein directly from DNA would be a little as if, as I write these words into my draft manuscript on screen, they are immediately transmitted to the publisher, printed, and sent to the bookshops. You have no idea what a disaster that would be. While the analogy is loose, the crucial stages of editing, reformatting, cutting and pasting, and proofreading are an absolutely vital part of the process. An RNA intermediary creates a buffer layer and gives flexibility and versatility to the readout process from the genome. It also allows many protein molecules to be produced rapidly from a single piece of DNA, a bit like making many copies of a book so that readers can all access the information simultaneously rather than each having to read the original in turn.

There may also be an evolutionary answer to the question. The fact that DNA can only be made with the help of proteins (such as DNA polymerase), and that proteins can only be made with the help of DNA, poses a chicken-and-egg conundrum for how the whole shebang could have got started when life on Earth began. Proteins are agents of chemistry: catalysts that enable reactions to happen with the exquisite molecular precision on which life depends. DNA, meanwhile, is a mere information-storage molecule, on its own of no more use than a cassette tape without a player. Without that information, those marvelous protein catalysts could never be made; the chances of anything useful appearing by the random assembly of amino acids is not significantly greater than zero. You need both of these features to get evolvable living systems, and each needs the other. RNA might break that deadlock. Insofar as it too can encode information in its base sequence, it can do DNA’s job—albeit not quite so well in the long term, as it is less chemically stable. RNA can also act as a chemical catalyst; in the 1980s biochemist Thomas Cech and molecular biologist Sidney Altmann found that some natural RNA molecules act as “RNA enzymes” or _ribozymes_, facilitating certain chemical reactions. This has led some researchers to suggest that RNA might predate both DNA and proteins in the origin of life, as a central component of chemical systems that could both replicate (and evolve by mutation) and conduct metabolic reactions. That idea is dubbed the RNA World. Only once DNA and proteins evolved did RNA get relegated to a subsidiary role, mediating the translation of genes to enzymes. In this view, RNA looks like a kind of vestigial evolutionary throwback. But even if the RNA World hypothesis is right, there are good reasons to think that RNA persists not just because evolution has been unable or unmotivated to dispense with it. For whatever that story—and in truth there is far more to kickstarting life than replication and catalysis—it is quite wrong to suppose that RNA took a demotion as a mere messenger for the information system of the cell.

RNA seems to be a general-purpose molecule that cells—particularly eukaryotic cells like ours in which gene regulation is so important—use to guide, fine-tune, and temporarily modify its molecular conversations. With this in mind, John Mattick has said that it is RNA, not DNA, that is “the computational engine of the cell.”

Far from being the mere messenger for making proteins, RNAs are the focus of much of the real action in our cells. Indeed, they are the _reason_ why many proteins exist at all. While proteins have traditionally been segregated into those that exist in compact, soluble form and those that sit within cell membranes, a third major class of proteins in our cells have the role of binding to regulatory RNA.

The transcriptional control of gene expression is a key aspect of the field called _epigenetics_. The term itself is venerable; it was first coined by Conrad Waddington in the 1940s to refer to any process affecting the expression of genes that does not involve a change to the sequence of the gene itself. Differences in the phenotypes of organisms may be related to differences in the sequences of their genes, as for example with differences in people’s hair or eye color. Or they could be due to epigenetic effects, where the genes themselves might be identical but the way they are expressed or read out is changed.

There is a “natural” sequence of epigenetic changes that accompany cell differentiation during the development of an embryo (embryogenesis), switching genes on or off. Switching can also be induced by the environment—for example, by stress, lifestyle, diet, or toxic substances.

Epigenetics rather blurs the distinction between “nature” and “nurture” insofar as those terms are proxies for genes and environment. For example, plenty of epigenetic control is exerted in the womb, so that identical twins can have different epigenetic “programming” at birth, as well as that acquired later from environmental influences.

Despite being an old and well-established idea, epigenetics has sometimes more recently been presented as a revolution that transforms the way we think about how life works. One sees claims that epigenetics rewrites all of biology, even Darwinian evolution, and shatters old dogmas. To an evolutionary biologist, however, epigenetics is of marginal relevance, because it does not appear to directly impact inheritance. The gametes—eggs and sperm—have mechanisms that protect their genomes from much epigenetic alteration, and strip away more or less all of it anyway as these cells mature into the state that takes part in reproduction. At the stage of gamete maturation called the primordial germ cell, epigenetic marks in the genome are largely erased. Could some epigenetic modification survive that process, though, and thereby be inherited? If so, it would constitute a kind of Lamarckian inheritance: a trait change acquired by an organism because of its experiences and environment and passed to its offspring. There is good evidence that this can happen, especially in plants—but it is probably rare in mammals. For example, some epigenetic changes to female mice with the yellow-hair-producing variant of the _agouti_ gene can be inherited by their offspring. But the evidence for some of the more dramatic claims of epigenetic inheritance in animals is equivocal. And in any event it does not seriously challenge the conventional Darwinian view of evolution, for there is no good reason to believe that inherited epigenetic marks on the genome can last for more than a generation or two before they are washed away.

Epigenetics is thus a central aspect of how life works, but is neither a new discovery nor a revolutionary one. Part of the hype surrounding the topic might be seen as an acknowledgment of recent genuine strides in understanding the details of the process. In part, too, it is doubtless entrained with the growing realization of how central gene regulation is to life’s molecular and genetic basis. But I suspect that at the root of the occasional overselling is a desire for liberation from notions of genetic determinism. Epigenetics offers a way to readmit the environment and experience as determinants of our fate. But in an understandable desire to correct old myths about genes, we shouldn’t go to the extreme of denying their significance entirely and making epigenetics the new “secret of life.” The undoubted importance of epigenetics as an aspect of how (our) life works should come as no surprise. The idea that an organism that moves around and interacts socially with others and is prone to novel and unpredictable experiences should be controlled by a rigid program determining form and behavior makes no evolutionary sense: it would, you might say, be a bad way to design such an entity. It’s too slow to respond to change; one can’t wait for evolution to alter genes or rewire the genetic networks at its glacial pace. It is more effective to evolve mechanisms for producing rapid revisions of how the genes are used when circumstances demand it. Genetic hardwiring, with rather little regulatory control, is fine for bacteria. They are abundant and replicate fast, so they can evolve quickly in comparison to typical rates of environmental change. We have seen this all too plainly in the way superbugs have evolved to resist antibiotics over a matter of years. But we need something more agile in the way our cells operate.

There is a lesson here that we can take from the evolution of minds. In a sense, minds might be regarded as the highest-level counterpart of gene regulation as a mechanism for rapid adaptability that doesn’t depend on the evolution of genomes themselves. Rather than changing our genes, minds create the option of changing our _behavior_—of finding innovative new solutions to problems on the hoof. We might say that minds are the nuclear option. They require a truly enormous investment, for brains are hugely expensive in terms of energy use, and so only become cost-effective as organisms get bigger and more complex. This creates a kind of bootstrapping acceleration: the more complicated a mind gets, the more it opens up new opportunities and niches that also bring fresh challenges to be faced. This solution is _cognitive_, in that it involves the organism collecting, processing, and integrating information. But as we will see, it is meaningful to regard lower-level adaptive processes in biology as cognitive too—even in the way single-celled organisms operate. Life is, as biologist Michael Levin and philosopher Daniel Dennett have argued, “cognition all the way down.”

### Proteins: Structure and Unstructure

The prevailing view has long been that a protein’s structure dictates its function. Typically an enzyme has only one job, and is shaped to be is extremely good at it. Other proteins are _structural_: they don’t function as enzymes but, rather, as components of the cell’s fabric. These molecules don’t tend to fold up into blobs, but instead intertwine or stick their chains to one another to make fibrous structures. They are the constituents of hair, bone, claw, and silk. During the 1960s, thanks in particular to the work of molecular biologists Walter Gilbert and Mark Ptashne, it became clear that some proteins act not to catalyze reactions but to regulate genes.

I call the above picture of protein structure and function “canonical” because it was long regarded as the alpha and omega of proteins, summarized in the rubric that “sequence dictates structure dictates function.” The key to the way proteins work, in this view, lies in their exquisite molecular architecture. But over the past two decades it has become clear that this is only a partial picture: it applies to some proteins (to some degree), but not at all to others. And it fails for some of those that are most important for how our cells work and how our bodies and our health are regulated at the molecular scale.

Because many proteins won’t form crystals, we only know the structures of about 50 percent of the human proteome (that is, of the entire complement of our proteins). The rest are a mystery, sometimes called the “dark proteome.” Many of these have sequences that don’t look like those of known proteins, so it’s hard to guess how they fold. Among the proteins that don’t yield to structural analysis are many that simply don’t have well-defined folded shapes. Instead, parts of their polypeptide chains are loose and floppy—“intrinsically disordered,” in the jargon of the field. Intrinsically disordered proteins are not strongly committed to a particular “shape”: in contrast to traditional globular proteins where there is a uniquely stable folded shape into which the collapsing chain is channeled by a wide “energy funnel”, their energy landscape is more randomly rugged, with several competing conformations they may adopt. This is a rather common phenomenon: one estimate puts the proportion of disordered segments in the entire human proteome at 37–50 percent. What’s more, disorder seems particularly prevalent in many of the most important proteins in the molecular ecology of the cell.

This way of interacting seems like the antithesis of what is typically taught in molecular biology: that proteins and other biomolecules like to form highly specific, interlocking partnerships. The more we look, the more it seems that this neat story about molecular recognition in the cell stems from the fact that it is easy to tell and to intuit, and that our attention has been drawn toward proteins that fit the story, than that it is the way life actually works at this scale. Fuzziness and imprecision are not only common; they are features of some of the most important molecular unions in the cell. Many of the proteins that play central roles in orchestrating molecular events in our cells seem explicitly shaped by evolution to be promiscuous, binding several similar partners with more or less equal avidity. This is not a mistake or shortcoming of evolution, but an intentional feature.

Biochemists Vincent Hilser and Brad Thompson have proposed that disorder is a better way to ensure that a change in one part of a protein can be felt in another part, because it doesn’t require precise “engineering” of a mechanism to couple them. Binding of a ligand produces subtle shifts in the many different conformations to which a disordered protein has access, and these collectively “spread the word.” The floppiness makes such proteins not only able to bind several different ligands but makes them intrinsically and broadly sensitive to what is happening in different parts of the molecule so that the binding event can have knock-on consequences. In this way, disordered proteins can become versatile “connectors” between different chains of interaction that convey signals in the cell, making them excellent hubs in the networks of molecular interactions involved in signal transduction and regulation. Even though—indeed, precisely because—intrinsically disordered proteins interact only weakly and transiently with many other molecules, says Polish cancer researcher Ewa Grzybowska, they enable cells to respond quickly to a change in circumstances, giving access to a wide variety of possible routes for transmitting and directing signals that are—and this is crucial!—not preprogrammed into the system.

It’s ironic, really, that just at a time when experimental and computational tools for deducing the structure of proteins have acquired a new sophistication that overcomes many of the limitations of traditional methods (such as the need for large protein crystals), we are starting to realize that protein _structure_ is not as central to protein _function_ as we once thought—and in particular, that many of the proteins whose regulatory roles truly distinguish us and other complex animals from simpler forms of life rely on their dynamism and disorder.

There is a loose analogy with the way the computational architectures known as neural networks operate. These are the systems used for machine-learning algorithms such as AlphaFold, and they consist of interconnected “nodes” at which several input signals from other nodes are integrated into a single output. None of the nodes’ functions are specified in advance by the programmer. Rather, they are acquired by “training” the network to give the right output to many data sets: to correctly identify pictures of cats, say, in digital images containing them. It is often very hard to say what the “function” of any given node in the network is after the system has been suitably trained. But it is certainly meaningless to ask what its function is _before_ training; its role is established only through “experience.” If the set of training data is different—dogs, not cats, say—then the way each node performs will be different.

The analogy is far from perfect, not least because proteins don’t need to be “trained” to acquire their roles: cells could hardly survive if they did. But those roles arise from a complex interplay of the “information” they possess by virtue of the genetic sequence from which they were derived, the processing that takes place during transcription of their mRNA (such as exon splicing), any post-translational chemical modifications, and the way they interact with other proteins and molecules in their vicinity, facilitated by the versatility that disorder supplies. You might say that the proteins have to wait to be told what to do. In this way, the states and operations of the cell are selected through a combination of prescribed “bottom-up” information along with inputs both within the cell and outside it. (And we mustn’t forget too that even the “bottom-up” information from genes gets regulated epigenetically from the top down.) In other words, the information ecosystem within which proteins operate is not that of a _closed_ genetic blueprint, but is _open_. It makes no sense to suppose that any given level of this complex system is more “in control” than any other. We must think about how all this works in a different fashion.

### Networks: The Webs That Make Us

Whatever their exact nature, transcription hubs show biology working at the molecular level in a very different way to the conventional picture of precise and specific molecular interactions directing cell processes like pieces of clockwork. “Many of the textbooks and even our language conveys this kind of factory-floor image of what goes on inside of a cell,” says Brangwynne. “But the reality is that the computational logic underlying life is much more soft, wet and stochastic than anyone appreciates.” The molecular decisions are made by rather ad hoc committees consisting (or so it might seem) of whoever happens to be around. Hyman has compared them to flash mobs: they congregate when the music is on and disperse when it stops.

In the end, what we are talking about here is the management of information: how it flows, how it is combined and integrated, and how it copes with the unexpected. If life’s informational schemes are overspecified, their outputs are liable to be brittle: too fine-tuned to withstand variations and noise. What’s required instead is _robustness_—which in general demands adaptability and flexibility. It requires not a concentration but a _dispersal_ of power. Gene regulation by condensate hubs might be influenced at several levels: for example via changes to the segregating propensity of proteins (by transcriptional control of alternative splicing) or of chromatin (by epigenetic markings to DNA and histones), or via changes to expression levels that alter the mix of RNA and proteins in the nucleus, or via the transcription of particular noncoding RNAs. Genes themselves play a role in such things, but it’s not obviously meaningful to ask who is ultimately in control. The process works as an integrated whole.

Just as gene regulation is better seen not as a series of switching operations in a fixed circuit but as a more fluid and collective process of consensus-reaching among many molecules, so the signaling within protein interaction networks no longer appears to have a precise “digital” logic to it, but follows principles that seem uniquely biological.

It’s possible that making promiscuous, reconfigurable networks doesn’t just convey advantages but perhaps is the only way a complicated system like our cells _can_ work, if it is to be robust against ineluctable randomness and unpredictability in the fine details. The operational principles exemplified by gene-regulating condensates and signaling pathways like the BMP system—with their molecular promiscuity, their combinatorial, fuzzy logic, their versatility for addressing and promoting many different cell states, and their apparent evolvability—may well be a sine qua non for multicellular organisms in which genetically identical cells work together in diverse, specialized states. This principle might even have been the innovation that allowed beings like humans to emerge. Such insensitivity to fine details is, after all, a much wider aspect of how large, multicellular organisms work: you can see it in the operation of the brain’s neural networks, the way cells assemble into tissues and the ability to accommodate gene mutations and deletions. It looks like an effective general strategy for robustness.

In short, then, much of the cell’s biochemistry might be far less sensitive to the fine details than has been thought. It’s not exactly that details don’t matter—but that we can’t easily tell a priori which do and which don’t. That, in a nutshell, might be said to be the central challenge of molecular biology. With so much detail at the molecular level, it’s tempting to suppose that _all of it_ is crucial to what happens at higher levels. But as gene knockout experiments show, often it isn’t. Again, what matters is not how each member of a molecular committee made its deliberations, but the collective decision that emerges. The apparent chaos of interacting components is in fact a sophisticated system that can process and extract information reliably and efficiently from complicated and contingent cocktails of signaling molecules.

Research into complex systems has suggested how best to think more broadly about these operational principles of molecular and cell biology: the goal of these principles is to take causation largely out of the hands of the molecules themselves. To reliably produce some larger-scale response, whether it be the fate of a given cell, the cellular collaborations that make tissues and organs, or an organism’s behavioral reaction to a stimulus, it is preferable to match the scale of the cause to the scale of the effect. Complex organisms seem wired to enable this so-called _causal emergence_. Emergence refers to the appearance of overall behavior in a complex system of many parts that can’t be predicted or understood by focusing just on what those parts themselves are like.

Some researchers, though, are suspicious of this notion of emergence. It seems to them an appeal to quasi-mysticism, as though a magic ingredient has been added to the system beyond the forces and exchanges of matter and energy between its constituents. They might assert that everything the system does can still be understood from a bottom-up dissection, given enough computational power to put all the pieces together. But the idea of causal emergence doesn’t deny that a system can be dissected into its constituent elements, nor does it require anything to be added to their interactions. Rather, it asserts that we can’t speak of the _cause_ of the large-scale behavior as originating primarily among those microscale interactions. To talk about causes in a meaningful sense, we have to acknowledge that there are higher-level entities and influences that must be recognized as every bit as real and fundamental as their constituent parts, and not just as arbitrary ways to aggregate them. _These_ are the causes of what transpires—and they are not just the sum of lots of microcauses.

While causal emergence seems to be a general design principle for life, it is rarely evident in our own technologies. Machines tend instead to use simple chains of causation: this cog turns that one. Remove a cog and the machine stops. Arguably something more emergent is now evident in computing technologies. We tend (rightly) to imagine that the causal explanation for what happens on our screens is to be found in the algorithms and machine code used to program them, rather than in the disposition of electrons in transistors. But it’s striking how some efforts to improve computing, particularly to create more versatile and powerful artificial intelligence, focus on making it more like our own neural circuitry, whether that is by using so-called artificial neural networks for machine learning, producing “neuromorphic” circuitry that can reconfigure itself like our brains, or giving advanced AI human-like attributes such as internal representations of the world. In other words, maybe the better computers of the future will be more causally emergent.

The work I’ve described in this chapter, however, seems to suggest that what changed over the course of evolution is nothing less than the _locus of causation_. I have called this _causal spreading_. So if we want to understand the mechanisms behind some key evolutionary shifts—for example, the emergence of complex body shapes and lifestyles in the Cambrian explosion, the emergence of nervous systems and of new modes of cognition, and the divergence of mammals and other vertebrates—genomes are the wrong place to look. All we can hope to find there are echoes of the true causal factors that happened at a higher level of organization—in particular, changes within the networks of interaction between and regulation of the molecular components, and changes in the ways cells themselves interact, stick, and collectively transform one another. I am convinced that the same story goes for the emergence of human-level cognition.

I think we will find that what gave us cognitive abilities no other species possesses—in particular, the ability to develop language, to think abstractly, and to maintain highly nuanced social interactions—is a change at a higher level than the genomic: a change in the emergent properties of the brain. No single gene “made us human.” What makes this story so complicated—and has hindered us from seeing it sooner—is that the migration of causation to higher systemic levels is not total; hence “spreading,” not “shift.”

### Cells: Decisions, Decisions

You’ll no more get a human from a random mass of cells than you’ll get a functioning business by collecting a hundred people off the street and gathering them into an office block. Before anything useful can emerge from such a crowd, the people will need to talk to one another, to be assigned specialized tasks, and to operate in collaborative fashion. Something much like this occurs as our bodies grow by cell multiplication from an initial single-celled zygote (the fertilized egg). Different cells take on specialized roles, developing into particular tissue types that each have their own designated shape and position in the body. In the end, this community of cells may have members that look very little like one another, having been shaped for their specialized jobs.

The growth of an embryo is like the growth of a city: it’s the growth process itself, not some blueprint, that gives it form. What happens next depends on what happened previously. This produces a delicate dance of contingency and inevitability. All cities, like all people, are different in detail, but mostly similar in broad outline: their transportation networks, say, or the disposition of the business and residential districts, tend to have similar features. If we could grow London again from a Roman settlement, it would not look identical, but it would probably have a remarkable amount in common with what we see today. What this implies is that there are some general principles governing urban growth that don’t specify the details but only the broad, generic outcomes. Those principles aren’t somehow encoded in the houses and streets, nor even in the minds of architects and civil engineers. They are an emergent characteristic of the way those constituent elements interact. So it is for cells and tissues.

Single-cell RNA sequencing has shown that, as cells become committed to specific fates, they develop distinct patterns of gene activity. That’s to say, there are a limited number of distinct profiles of transcription—and as embryo growth proceeds, these evolve toward those found in mature cell types such as the neurons of the spinal cord or blood cells. A complete description of that process would correspond to some immensely multidimensional plot, with each axis corresponding to the concentration of a different RNA molecule. But happily, the data collected from scRNAseq is generally amenable to dimensional reduction: the unthinkably complicated high-dimensional plot can be projected onto one with a small number of—perhaps even just two—axes. Each of those axes can be thought of as some abstract and complicated mashing-together of the concentrations of many different RNAs. Reducing the dimensionality in this way loses some of the information in the full data set, just as we don’t see the full shape of a 3D object from its 2D shadow. But it captures the key details. In particular, it can reveal what are in effect the Waddington valleys that correspond to different cell fates—and the trajectories that cells take to reach them. We can, in other words, map out the cell-fate landscape, and how the cells in a developing organism populate it at different instants. Thus researchers can plot these patterns in diagrams that show how a pool of initially identical cells splits into separate streams leading to, say, the blood, forebrain and hindbrain, and eye.

The cell-fate landscape seems to be a rather robust object that transcends details of the genome. It is defined rather by the networks of interaction between DNA, RNA, and proteins, from which certain stable communities of molecules emerge. And it defines what is possible for the organism at the level of cells. You might say that, whereas the genome provides the basic resources for making a cell’s repertoire of biomolecules, the transcriptional landscape—an emergent feature of the interactions between those molecules—defines the cellular resources that are available for making the organism. And just as the genome constrains but doesn’t meaningfully _encode_ these cell states, so the repertoire of cell types constrains but doesn’t _encode_ the tissues and bodies that can be made from them.

Researchers are now beginning to understand the principles governing these changes in cell state. The proper language to describe them is borrowed from a branch of mathematical physics called the theory of dynamical systems, which is used to understand processes of change in systems that have many interacting components. The concepts are most easily illustrated for systems that have only a small number of components rather than the many thousands of molecular constituents of cells. The archetypal example is the solar system, comprised of a star and planets that interact with one another by the force of gravity.

Imagine if Earth were the only planet in our solar system. That’s a very simple dynamical system, and the equations of motion (based on Newton’s laws of mechanics) have a simple solution: the Earth orbits the Sun. This is a stable dynamical state: if another star happened to pass close by and temporarily tug the Earth slightly out of its elliptical orbit, it would return there once the disturbance had passed.

Now let’s add into the solar system another planet: Jupiter, a gas giant about three hundred times more massive than the Earth. Each of the three objects feels the gravitational tug of the other two, and it’s not immediately clear what state they’ll settle into. In fact, Isaac Newton realized that even for just three celestial bodies like this, it’s not possible to solve his equations of motion to work out the trajectories. We do still know what the outcome seems likely to be, however: the Earth and Jupiter will orbit the Sun. But there are now subtle “three-body” effects on movement: the shape and position of the Earth’s orbit, say, change very slightly over time because of Jupiter’s influence.

If we now add in all eight planets, not to mention Pluto, the asteroids, and planet-sized objects like Ceres, it all gets terribly complicated. We think of the solar system as being a stable dynamical system with each planet revolving along its orbital track, but in fact these orbits are constantly changing in small ways—for example in the eccentricity or elongation of the elliptical orbits—as time progresses. The dynamics are too complicated to work out with pen and paper, and all we can do is simulate the system on a computer and see how it plays out. Such studies seem to suggest that the current arrangement of the solar system isn’t in fact stable indefinitely but could eventually break up. What’s more, the outer gas-giant planets didn’t always have the orbits they have now, but may have gradually migrated there from a slightly different arrangement in the early solar system. It’s a very complicated dynamical system, and constantly changing.

If that’s so for the solar system, how much more so for the cell! Yet the solar system clearly has a rather stable arrangement just now, which is also robust against small disturbances: if some cosmic force were to alter the planetary orbits a little, they would gradually return to their current positions. In the language of dynamical systems, this state of the solar system is called an _attractor_. It is like a valley in the dynamical landscape: push the system slightly away from this stable state and it will return.

Attractors are the modern equivalent of Waddington’s valleys. The point about his landscape metaphor was not just that differentiation takes a cell lineage down one of several possible routes in a series of bifurcations, but that it does so in a way that is insensitive to details. As with attractors, small deviations are tolerated. Almost by definition, this canalization implies the removal of causation from the genetic or molecular level to another, higher organizational tier. It seems to be a general property of complex, multicellular organisms.

What happens, though, if you disturbed the solar system _a lot_—say, adding another Jupiter-sized planet in place of Venus? Then you might find that several planets migrate to quite different orbits: there is a new landscape, and a new attractor state. Maybe you don’t even need to alter the components, but just their arrangement—totally shuffling all the existing planets. You might find that they settle into a quite different pattern. This is like kicking Waddington’s ball so hard that it passes over the top of a ridge and into a neighboring valley.

We saw that a cell’s state can be altered by signals (such as hormones) arriving from outside at the cell surface, which may trigger a signaling pathway inside the cell that produces some change in gene expression, which in turn could flip a permanent epigenetic switch. That is basically what happens when cells make fate decisions, and researchers have shown that it can be described using the tools of dynamical systems theory.

In effect, then, cells decide on their fate according to their location in _three_ types of space. First, they occupy our familiar three-dimensional space, having specific locations in the tissues of the embryo that determine which neighbors a cell has and what signals it receives. Then there is a location in the gene expression or transcription space, which reflects what kind of cell it is: how stem-like, say, or how mesoderm-like. And the developmental trajectory that the cell lineage takes depends on where it is in the “decision space” of valleys and bifurcations.

Noise, coupled perhaps to antagonistic interactions in gene and protein networks that can produce amplifying feedback mechanisms, can be exploited to create and sustain a useful variety of cell types. That seems likely to be what happens in cell-fate decisions during embryogenesis too. Some transcriptional noise in the pool of cells that haven’t yet fully differentiated on the transcriptional landscape keeps them in an ambiguous state, their options still open so that they can go down various channels as the circumstances dictate—and perhaps even change course at a rather late stage if needed. From the genetic to the behavioral level, life _needs_ noise because it can’t afford to be too deterministic. You never quite know what lies around the corner.

The fates of our cells, and the nature of our tissues and bodies, are apparently far less inevitable and inexorable than was previously thought: living matter is plastic and (re)programmable. Cell reprogramming is now being explored for regenerative medicine.

The plasticity of cell fate—the possibility of altering it by providing molecules that can rewire the interaction networks—forces one to wonder: are the cell-fate maps that we have plotted so far an exhaustive picture of the possibilities? Or might there be new valleys, new cell types, that could in principle be attained from the interactome networks but which are simply not used in normal human development?

I have talked here about cells _deciding_ their fate: _electing_ which valley of the landscape to go down. This sounds like very anthropomorphic language, but it needn’t be. After all, we speak routinely of computer systems making decisions too, especially in artificial intelligence. An AI system like IBM’s Watson makes business recommendations and medical diagnoses on the basis of provided information. And cells surely perform computation of a sort, taking in information from their surroundings and integrating it to generate an appropriate response. But this is rather different from a computation carried out by a silicon chip, as cells are constantly reconfiguring their actual “circuits” and incorporating new components. Their operations are adaptive and contingent, as well as noisy, fuzzy, and error-prone.

In short, says biologist Dennis Bray, the cell’s circuitry (if that is even a good metaphor at all) “is a long way from a silicon chip or any circuit a human would design.” The more we learn about living systems, Bray writes, “the more we realize how idiosyncratic and discontinuous they are” relative to computers. In particular, he says, “living cells have an intrinsic sensitivity to their environment—a reflexivity, a capacity to detect and record salient features of their surroundings—that is essential for their survival.” This feature is “deeply woven into the molecular fabric of living cells.” A primitive _awareness_ of the environment, Bray suggests, was an essential ingredient in the very origins of life. He calls cells “touchstones of human mentation”: a kind of minimal model of what cognition can and should mean.

Here again, then, is Michael Levin and Daniel Dennett’s notion that life is “cognition all the way down.” “The central point about cognitive systems,” they say, “is that they know how to detect, represent as memories, anticipate, decide among and—crucially—attempt to affect.” Cells can do all of this. British computer scientist Richard Watson and coworkers have shown using a computer-simulation model that the process of cell differentiation, involving the reconfiguration of the cell’s interactome, is formally equivalent to the way a neural network performs a learning task, integrating information from multiple sources to come up with a decision. In effect, the researchers say, this is a categorization task: the cell needs to figure out which cell state is appropriate to a given set of inputs. Watson and colleagues showed that collections of cells can attain complex developmental targets by learning rules that become encoded in their regulatory networks. This, the researchers say, is a powerful way to rapidly identify good adaptive solutions to the demands of a given environment. In other words, “evolving gene networks can exhibit similar adaptive principles to those already familiar in cognitive systems.”

Cognitive systems exist to integrate information from many sources to produce a goal-oriented response. To that extent, all living systems _have_ to be cognitive agents almost by definition. And the fact that evolution grants this capacity should really be seen as unremarkable, for cognition is clearly a good way to deal with the unforeseen: to develop versatile and instantly adaptive responses to circumstances an organism has never encountered in its evolutionary past. The more complex an organism is (or perhaps it’s better to say, the more complex and unpredictable its environment is), the more cognitive resources we should expect it to have. I don’t anticipate a consensus any time soon on the question of how to define life, but it seems to me that cognition provides a much better, more apt way to talk about it than invoking more passive capabilities such as metabolism and replication. Those latter two attributes might be necessary, but they are means to an end: they’re not really what life is _about_. And the fact that life has an _aboutness_ at all is intrinsic to what it is.

### Tissues: How to Build, When to Stop

How does the featureless blob that is the early embryo know what to make, and where to make it, if not by reference to a blueprint? The answer is that cells produce order and form by dialogue: by communicating with and responding to one another. Each is bounded by a membrane studded with molecules capable of receiving signals arriving at the cell surface from outside and converting them into messages within the cell’s own networks of interacting molecules. This signaling typically leads to the activation or suppression of specific genes, changing the internal state of the cell.

There are three main modes for transmission of these external signals. One is chemical: a molecule outside the cell diffuses to its surface and binds to a protein receptor there, triggering some change on the inside of the membrane that begins a signaling cascade in the interior. That, for example, is what happens at the junctions between neurons called synapses. One neuron, triggered by an electrical pulse that has traveled to the synapse along the threadlike axon, releases a neurotransmitter molecule such as dopamine or serotonin. This diffuses across the gap that separates neurons at the synaptic junction and attaches to the surface of another neuron. Typically, the arrival of a neurotransmitter either excites the neuron to produce its own electrical pulse, or inhibits it from doing so. Hormones also act as chemical messages registered by cell-surface receptors.

The internal activity in a cell can also be altered by mechanical signals: by its membrane being stretched, say, as another cell sticks to and pulls it. Typically, these mechanical signals are “transduced”—converted to some internal effect—by membrane proteins shaped like tubes or pores. When pulled or squeezed, they may open or shut to admit or exclude electrically charged ions from passing across the membrane and into the cell. Because of such mechanosensitive behavior, a cell culture might develop differently when grown on a soft surface (which can deform in response to the developing tissue) or a hard one (which cannot).

The third mode of cell communication is electrical. Because of their ability to control the passage of ions across their membrane, cells can become electrically polarized, for example having an imbalance of charge (that is, a voltage) across the membrane. That’s how electrical pulses travel along the wire-like axons of neurons in the first place, via the shuttling of sodium and potassium ions across the membrane. This is made possible by proteins called ion channels: pore-like structures that permit passage of water-soluble ions through the fatty interior of the membranes. Ion channels are generally selective about which ions they allow to pass—some, for example, will admit sodium ions, others potassium. In this way cells can control differences in charge on each side of the membrane—which is to say, the voltage across them.

From a confederacy of cells, the shape and form of tissues and bodies thus emerge not by some painting-by-numbers plan that inserts each cell in its allotted place but, rather, by a bootstrapping growth process of successive elaboration. As each new form emerges, it supplies the context for the next stage. Much of this process involves mechanical forces: sheets and other cell aggregates may buckle and fold as they expand within the constraints of their surroundings, or cells segregate and configure through differences in the stickiness (adhesion) that one cell type feels for another. Shape here becomes both a cause and an effect.

Even if we know the basic rules of morphology, we still struggle to understand or predict how they will play out. It’s a subtle process, involving the interplay of information at the scales of the whole organism, the genetic and molecular activity in its cells, and everything in between: a complex mixture of bottom-up, top-down, and middle-out signaling. In this way, says Michael Levin, “the same cellular machinery can build one of several anatomies based on specific information-bearing physiological states”: there is _plasticity_ in this system, scope for variation, improvisation, and adaptation. If one small thing goes awry, tissue growth can often accommodate it without getting thrown off course.

One thing does seem plain: life doesn’t make systems that can do or construct a single thing, but produces entities—cells and their genetic, transcriptional, and protein networks—that embody a wide range of options. The trick it must master is then to ensure that, in normal circumstances, the system converges on the outcome(s) favored by natural selection, _while still maintaining enough phenotypic variability to be evolvable_. Evolution does not know, so to speak, what it is going to need tomorrow—so it must keep options open. Such variation is possible precisely because the rules by which development unfolds are _loose_ ones. If there’s a central feature of how life works, it is surely in this ability to create outcomes that are neither arbitrary nor wholly prescribed.

The rules that govern life are, then, not _prescriptive_ but _generative_.

### Bodies: Uncovering the Pattern

The key question for understanding development is how a cell knows where it is in the body, from which it may deduce what it should become. Such positional information can be delivered throughout a tissue by a _gradient_ in the concentration of some signaling protein, the expression of which is switched on in one place and which then diffuses through the tissue. It’s a little like figuring out from the intensity of smell how close you are to the kitchen.

Concentration gradients of diffusing morphogens provide a very general means of delivering _positional information_, enabling cells to differentiate to distinct fates depending on where they sit in relation to the morphogen source. In this way the morphogens can distinguish an axis of symmetry within a developing embryo, whether in the whole body or just a specific region such as a limb or organ. Complex structures can thus be gradually built up by progressive elaboration of an initially rather featureless mass of cells.

Turing’s mechanism of reaction and diffusion of morphogens offers a way to extend both the range and the organization of cell interactions beyond neighbors and across whole tissues. So once again we see that _nothing new is needed_ in terms of component parts for biology to achieve things at larger scales than the single cell. It can use preexisting systems, leveraged by the laws of physics and chemistry, to ramp up the complexity.

When the bones of the fingers have formed, they are still connected by intervening webs of tissue in the nascent hand. The individual fingers are freed by the disappearance of this tissue as the cells themselves die. This is an example of how body formation is not just a matter of adding tissue but of subtracting it: controlled cell death (apoptosis) plays a vital role. Once again, chemical signals from the surrounding tissues program the doomed cells, setting them on a course to apoptosis in which they die and are re-absorbed.

This is one example of how the body’s soft tissues adjust and accommodate themselves to the dictates of the bony skeleton. Isn’t it odd, for example, that whatever the size and shape of the limbs or body, all the tissues are consistently scaled and shaped accordingly? If people have limbs that are unusually short, or bodies that are smaller because of growth conditions such as dwarfism, the bodies and limbs are still self-consistent. Again, the only plausible way this can happen is for the cells and tissues to follow contingent, context-dependent rules, not some blind blueprint. Skin, for example, regulates its size by mechanical feedback: if the cells feel too much tension because the skin is getting taut, they proliferate to relieve it.

The formation of organs and bodies involves a delicate interplay between chance and necessity—a conversation between genes and genes networks, mechanical forces, and the environment. This is the beauty of the way bodies form: through rules that are designed to produce not a precisely defined end-product but something more generic.

It turns out that Turing’s patterning mechanism occurs in all corners of the physical world, and not just in developmental systems. It may operate even at the ecosystem level in the way some ants deposit the bodies of their dead in well-spaced piles, or how grass becomes patchy in semi-arid terrain. The same basic process of activation and inhibition has been invoked to account for the ripples in windblown sand and for structures seen in the solidification of metal alloys. That Turing structures are used to shape the body, then, might best be regarded as one among several examples—including liquid-liquid phase separation, segregation, and fluid flow—of how nature does not so much work from scratch, inventing bespoke solutions, as exploit the affordances of the physical world. Physical laws are not suspended in living matter; evolution harnesses rather than subverts them, and in this way can sometimes get order and organization “for free.”

Body-patterning processes based on the diffusion of biochemical morphogens are ubiquitous for complex organisms, suggesting that evolution has capitalized on them as a useful tool for getting large-scale organization from molecules. One reason is surely that they produce structures at much bigger scales than the molecular or the cellular. Another seems to be that they can be very robust: small, or sometimes even rather big, changes in the molecular details don’t really alter the outcomes. This means that organisms are free to mutate and evolve at the genetic level without totally screwing up their ability to produce dependable forms at the scale of the whole organism. Here again we can see one of nature’s design principles: to find the right balance between top-down, bottom-up, and middle-out mechanisms for building organisms, so that adaptation and variation can happen, and innovations—dramatic new solutions to the challenge of “design”—are possible without producing a dangerous sensitivity to small changes. The forms that spontaneously emerge create a generic chassis on which to build and experiment. We don’t know the gamut of possibilities available to those experiments. They are surely wider than nature has so far found room for.

This picture of body patterning suggests too a new perspective on the phenomenon of _convergent evolution_, which refers to the way organisms end up with similar features or body shapes through independent evolutionary pathways. Convergent evolution is often regarded as a sign that certain shapes or structures are ideal adaptations to particular environments for physical reasons: wings consisting of flat, thin membranes are best for flying, torpedo-shaped bodies are streamlined for efficient swimming, and so on. The idea is then that evolution will find its way independently and repeatedly to a “good” engineering solution. There is likely to be some truth in this, but it’s also possible that sometimes the same forms recur independently in nature because they are the only ones generically permitted by the physical mechanisms of their generation. There is a tendency in evolutionary biology to regard natural selection as a process with an infinite palette: anything is possible so long as it doesn’t break the laws of physics. But the laws of physics might impose more constraint than that, precisely because biology uses rather than merely suffers them. Both the zebra and the zebrafish are striped nobecause stripes are the pattern best suited to their circumstances but because stripes are a generic pattern that arises from a Turing activator-inhibitor scheme. Maybe, says Newman, “evolution ‘selects’ preexisting, generically templated forms, rather than incrementally moving from one adaptive peak to another through nonadaptive intermediates.”

### Agency: How Life Gets Goals and Purposes

One of the big challenges for biology is to develop a rational, productive framework for understanding concepts such as _agency_, _information_, _meaning_, and _purpose_. These are not optional add-ons for the philosophically inclined, once we have solved all the minutiae of how life works at the microscopic scale. Rather, they sit at the core of life itself. Without that big picture, we risk ending up with the equivalent of a detailed description of everything about a complex machine’s operation except for an understanding of what it actually does. What, after all, is the point of knowing how life works if we don’t know what it is working toward?

Living organisms are nonequilibrium systems. Our hearts are constantly beating, pumping blood around the body. We are reflexively breathing in and out, without trying or even noticing. Our brains, even when resting, are a miasma of signals zipping through the neurons’ tangled network—the living brain is never quiet. Every cell is burning up energy to drive its metabolic processes. We depend on a constant throughput of energy—for us it comes in the form of energy-rich food, but ultimately the energy source for nearly all life on Earth is the Sun, which powers the growth of plants at the bottom of the food chain. And you had better hope that you stay out of equilibrium for as long as you can—for equilibrium means death.

Does this mean that life is just a particularly complex nonequilibrium phenomenon—that we are, so to speak, intricate whirlpools of matter and energy, spinning for a glorious instant until the mortal coil unspools? It’s an attractive notion (well, I find it so; some might see it as terrifying or soulless), but I don’t think it is quite right. For one thing, we are not each spun afresh like dust devils—we each have within us a deep evolutionary memory embodied in our genomes. If these are not blueprints, they nevertheless do encode information shaped and inherited over eons that is indispensable to our formation. Life is, in other words, a _learned_ affair.

To what extent life is dictated by this Darwinian memory and to what extent it can draw on spontaneous ordering mechanisms is one of the central questions for understanding how it works. We’ve seen that we are a bit of both—but I do believe that our evolved nature creates a fundamental distinction from other examples of nonequilibrium order, not least because it is what laces our structures and behaviors with purpose and meaning—and with their expression as _agency_. We are, you might say, dust devils with goals, actively sustaining and maintaining ourselves while spinning off progeny that do the same. In the details of how that process occurs, thermodynamics still rules: we can’t escape its laws. But a thermodynamic description is not enough to encompass life: we are a qualitatively unusual sort of nonequilibrium system, and until we have a theory that accounts for that difference, we will not have a proper understanding of how life works.

By agency, I mean the ability of a living entity to manipulate and control itself and its environment in order to realize a goal: to achieve a purpose. Agents can do this because they are able to ascribe—I don’t necessarily mean consciously, but in some way that registers internally—meaning to self and surroundings (including other agents). They can _attend to what matters for them_. They experience their environment not as a neutral succession of interactions and events but in terms of _affordances_: useful possibilities for attaining a goal. Our own experience of meaning in our lives is an evolved elaboration—a wonderful and rather perplexing elaboration, often seemingly rather remote from any adaptive Darwinian agenda—of this ability in all biological agents.

This is really what makes living things different from other self-organizing nonequilibrium structures: life ascribes values and meanings and has goals. To put it another way, living things are not nonequilibrium structures that happen to resist the disordering pull of entropic decay; they are structures that have evolved capabilities specifically to do so (and which I believe are best regarded as cognitive capabilities). We need to understand how they manage that.

Maxwell’s demon therefore exhibits its agency, overcoming (if only temporarily) the randomizing, disordering tendency of the molecular world, because of four characteristics:

1. It has a goal.

2. It collects information meaningful for that goal.

3. It stores it in a memory.

4. It acts on that information to manipulate its environment in a goal-directed manner.

The modern analysis of Maxwell’s demon reveals a striking connection between information theory and thermodynamics: information (and here we really do mean meaningful information, selected for a purpose) and energy are interconvertible. That’s to say, by using information, the demon can build up a reservoir of heat (create some order) that can be used to do useful work. More properly, it’s not exactly information that serves as a fuel here, but having a place to put it. We can let the demon go on accumulating heat and doing work so long as we keep supplying it with _more memory_. Only when it runs out of memory must it produce entropy by erasing the information it already has.

What information must such a being gather in order to exercise agency in a noisy environment, so that it is not a mere cork battered this way and that at random? The key point is that an organism can make use of the environment by becoming _correlated_ with it. If you want to pass through a thronging crowd, you’ll do best to attune your movements to those of the folks around you—to dart into a space when it opens up, for example. Otherwise you’ll just end up uselessly colliding with others. If you want to take a journey, you’ll do well to correlate your arrival at the terminus with the bus schedule. And so on. Maxwell’s demon has to correlate the opening and closing of the trapdoor with the nature of the thermal fluctuations in the molecules that approach it. Likewise, if a bacterium swims dependably toward the left or the right when there is a food source in that direction, it will flourish more than one that swims in random directions and so only finds the food by chance. Many such correlations become established in the process of evolutionary adaptation: organisms are predisposed to act instinctively in a way well-suited to their circumstances, thanks to the ancestral memory recorded in their genomes. These correlations may also be engendered through learning from experience (we’re not hardwired to consult bus timetables).

So living organisms can be regarded as entities that attune to (correlate with) their environment by using meaningful information to harvest energy and evade equilibrium. Life can then be considered as a computation that aims to optimize the acquisition, storage, and use of such meaningful information. And life turns out to be extremely good at it.

This picture of complex structures (like organisms) adapting to a fluctuating environment allows us to deduce something about the way they store information. So long as such structures are compelled to use the available energy efficiently, they are likely to become “prediction machines.” It’s almost a defining characteristic of life that biological systems can change their state in response to some driving signal from the environment, in such a way that the chances of survival are improved. Something happens; you respond. Plants grow toward the light; they produce toxins in response to pathogens. These environmental signals are typically unpredictable, but living systems learn from experience, storing up information about their environment and using it to guide future behavior. They must implicitly construct concise representations of the world they have encountered so far, enabling them to anticipate what’s to come.

Geneticist Henry Potter and neuroscientist Kevin Mitchell have attempted to formalize the criteria for a system to act as an agent—meaning an entity with autonomous causal power. Without claiming to be comprehensive, they list several requirements for genuine agency. First, the agential system must be out of equilibrium with its environment—in other words, it must be _thermodynamically distinct_ from those surroundings. We have already seen how Schrödinger identified this as a feature of living organisms, and why it means that they must be taking in energy and dissipating some of it as heat. It’s why (except by coincidence) an organism will tend to have a different temperature from its surroundings. An entity that is in sustained thermal and thermodynamic equilibrium with its surroundings is not alive.

The corollary, say Potter and Mitchell, is that the entity must have a boundary of some kind: there must be some way of delimiting the region of thermodynamic autonomy. Within the enveloping membrane of a cell, it can maintain a state of affairs that is thermodynamically distinct from that outside: for example, creating and maintaining a difference in chemical concentration of ions and other components. Second, they say, an agent must persist for some meaningful duration of time. That’s a seemingly obvious requirement, but it reminds us that what characterizes a living organism is not the atoms it contains, which are constantly being exchanged with the environment, but its pattern of organization at a higher level. Agency is exerted not instantaneously but over periods of time.

In addition, an agent must have what Potter and Mitchell call “endogenous activity,” meaning it does things “for its own reasons,” not just in a stimulus-response manner. A piston has no endogenous activity: it just responds passively and predictably to a change in gas pressure in the chamber. To put it another way, what goes on _inside_ an agent is influenced but not fully determined by what happens _outside_.

A corollary of this criterion is that agents must have some internal complexity. A gene can’t have real agency because it lacks this “inner life.” Proteins might be better candidates for a kind of weak agency because their action depends on internal degrees of freedom: molecular vibrations and interactions among parts of the peptide chain, for example, that may convey allosteric activity. A genome has a still better claim to a degree of agency, although this too is weak because its behavior is so closely coupled to and dependent on events in the rest of the cell. It is really only at the cellular level that biological agency fires up.

Agents must also show “holistic integration”: they are more than the sum of their parts. We can usefully take an organism apart and look at the components—but we can’t truly understand what it does unless we put them back together again.

What most distinguishes true agents, however, is that they have _reasons_: all these other criteria are means to an end. Of course, this doesn’t mean that a bacterium reasons about its behavior in the way we do, through internal reflection and self-awareness. But it does mean that the bacterium is selective about what it attends to, because some environmental stimuli have survival value while others don’t.

For agents, _history matters_. Agents hold a memory of past events that may determine future actions. History has configured and primed them, embodying within them both goals and a pragmatic kind of “knowledge of the world” that directs their action. By doing so it has invested them with a causal power that can’t be reduced to the sum of its parts. Agents experience the world as a genuine web of _meaning_, which might be expressed in terms of _affordances_: how, given this state of affairs, might I best achieve my goal? What transformations of both self and environment might I effect to that end? What is useful to me in that quest?

Put this way, agency might sound very anthropocentric. But an enumeration of its characteristics in this manner can help us see how to _operationalize_ them: how to reduce them to components that might be quantifiable and measurable. One such conceptual framework is the _free-energy principle_, developed by neuroscientist Karl Friston and others, which suggests that agents act to reduce the difference between the state of affairs corresponding to their goal and the one they experience at a given moment. They may do so using an approach called “gradient descent,” which seeks the quickest path to the desired state—for example, a swimming bacterium following an increasing concentration of nutrient will select the trajectory along which the concentration gradient is steepest. And the proposal that agents embody holistic integration of information might be explored using a scheme called integrated information theory, developed by neuroscientist Giulio Tononi and others, which expresses a system’s ability to integrate information in terms of the shape of the networks of interaction among its components.

Having a mind is a good adaptive strategy for an organism that experiences a very complex environment. An alternative would be to equip the organism with a suitable automated response (that is, a response that will help to preserve it) for every stimulus it is likely to encounter. This might work tolerably well for a bacterium, the environment of which tends not to have very much diversity. But at some point along the axis of complexity, the amount of hardwiring required to deal with all the contingencies becomes prohibitively costly. Much better to give the organism a mind: a system that can receive information and generate responses not in any prescribed way, but by making use of features such as learning, memory, and an ability to imagine future outcomes. We are not the only organisms to have such systems, but ours is specialized in ways that others are not (and vice versa).

To portray a mind as a kind of high-powered survival machine orchestrated and controlled by genes is, then, fundamentally a misconceived picture of what minds are. When, in _The Selfish Gene_, Richard Dawkins expresses some surprise, bordering on affront, that minds/brains “even have the power to rebel against the dictates of the genes” (for example so that humans choose to have fewer children than they could, or none at all), he is mistaken about what the brain/mind is _for_. Of course it has arisen because of its adaptive advantages, and of course genetic (as well as developmental and environmental) influences have some say in how it works. But the point is that the ceding of control by genes is intrinsic to what a mind is—otherwise it’s not a mind at all, but just a fancy mechanism for an automaton. So when Dawkins says that no species has yet taken minds to their logical conclusion of issuing a single policy instruction—“do whatever you think best to keep us alive”—he paints the wrong picture. For that is in a sense already what genes have “done” with minds, except that because they retain some say in how the individual mind is built, they can bias the mind’s notion of “what is best.” It’s rather like parents who, knowing they cannot make every decision for their children given how unknown and diverse the children’s experiences will be, opts instead to train the children to use their own autonomy, informed by experience. “But remember,” the parents say sternly—“your goals are to survive and reproduce.” Years later the parents find a child working on the theory of quantum gravity. “What good is this for the goals we set you?” they demand indignantly. “But,” says the child, “the resources you gave me can do so much else. And right now, this is how I choose to use them.” The more complex a mind the evolutionary process makes, the less control it has over the mind’s goals and purposes.

I think we should regard purpose itself as an evolutionary innovation much like minds: one whose appearance was arguably coincident with the advent of the first true (agential) organism, and which is certainly present to some degree even in the simplest single-celled organisms we know of. Purpose can exist without minds, but it’s not clear how minds can exist without purpose. The real reason biology frets so much about irruptions of teleological thinking is that again these expose the fundamental lacuna: it can’t deal systematically with agency and so has to infuse it into entities as a kind of magical capability. I would go so far as to suggest that arguments about human free will persist (often rather tediously) because we lack any account of how agency arises, so that some suppose it can only be a causal fairy dust that fools think seeps into and among inert atoms and molecules to direct their motions.

Far too often, biology has been regarded as inventing everything from scratch. Nature, in this view, is a blank slate, an inert mass of atoms that must somehow be corralled and arranged into the intricate forms and patterns of interaction on which life depends. But the physical world is not like this at all. It offers all kinds of organization, process, and structure that life can use: phase separation, say, or dynamical landscapes, or self-organizing Turing structures, or the potential for entropic decline to be staved off and even reversed with a judicious use of information. It would be strange indeed if life did not exploit these features; indeed, we know that it does.

### Troubleshooting: Rethinking Medicine

For many conditions, genes and their protein products are simply not the right level of intervention—because, as we have seen in this book, they are not where “cause” arises. For the same reason, the much-vaunted notion of “personalized medicine”—where treatments are customized to the attributes of the individual—cannot rely solely on genetic information. To treat disease at its root, we have to identify the level in the hierarchy of life where that root is embedded. We have to attune the cure to the problem.

Genetic medicine will, then, be effective only to the degree that genes _do_ control health, which is somewhere between “a bit” and “somewhat.” If we want to intervene in some aberrant physiological process, we will do best to identify the level at which causation is most focused. As Topol has said, the root problem is the HGP’s implication—nay, assumption—that our genome sequence is our “operating instructions,” whereas in fact it is just “one layer depicting human uniqueness, and does not by itself reveal the depth of information derived from all the other layers that include the transcriptome, proteome, epigenome, microbiome, immunome, physiome, anatome, and exposome (environment).” Moreover, says Topol, many of these layers are specific to certain cell and tissue types or to certain sites in the body. The truth is that no disease is truly “genetic,” in the sense that disease always manifests at the level of the whole tissue, organ, or body; if it didn’t, we wouldn’t notice. Disease is a _physiological_ phenomenon. Genes aren’t necessarily the whole story even when they do seem to be the major cause of a disease—in monogenic conditions, for example.

Disease symptoms are typically less diverse than their causes. That’s precisely what GWAS studies tend to confirm even for individual diseases: more genomic loci are linked to a disease at the population level than are likely to be relevant for any one individual. What this shows is that our bodies have a relatively limited number of “failure/response modes,” which might be instigated by many different triggers. This canalization of the pathways of and physiological response to agents of disease—the convergence of different conditions into common “disease channels”—means that many different ailments can show remarkably similar effects.

Drugs designed to target a particular molecule won’t always strike at the true _causal_ root of a disease. By the same token, a drug that works against one disease might also be effective against another, if both conditions involve the same higher-level failure mode. That is the thinking behind the common practice today of _drug repurposing_, where drugs developed for one application are tested to see if they work for another. The candidates can include drugs that previously failed one of the hurdles of clinical trials—for, to even get to that stage, they must have shown at least some promise, and you never know where else it might prove valuable. If any of these old drugs are found to be effective for a new disease, they generally have the advantage that they will already have been tested in some clinical trials to assess their safety, and perhaps will already have been approved for use by regulatory agencies.

Some researchers would like to be able to track the immune system of individuals in real time to spot signatures of impending crisis. Glazier and others have called for a “moonshot” project to give everyone a computer-modeled “digital twin,” fed with data from real-time monitoring of our physiological status, that could be used to track our health, predict our response to treatments, and monitor that response in reality. Such a capability might take five to ten years to develop and would require huge cross-disciplinary effort. But the investment might be repaid many times over if, for example, it could help us navigate a pandemic by monitoring infection and the body’s response in real time. This is just one example of a broader movement within medicine to develop digital twins of individuals that can simulate our bodies at the cell, tissue, organ, and whole-body levels. Such schemes would set a fundamentally different goal for medicine, Glazier says. Currently, he says, “instead of trying to regulate outcomes, we wait for things to happen and then react.” But what he calls “engineered health” or “closed-loop medicine” would seek instead to maintain the body’s status quo through constant physiological surveillance and guidance, informed by predictive models of the effects of afflictions and interventions. It would be less a matter of curing disease, and more of curating health.

If cancer is at root a matter of cells falling into the “wrong” state—the wrong basin of attraction—perhaps the real goal is to get them back out again. That might have more in common with the kind of cell-state engineering involved in stem-cell research than with developing drugs against molecular targets. It’s about redirecting life itself to new destinations.

### Making and Hacking: Redesigning Life

Bodies aren’t arbitrary constructs. Neither, though, are they fully specified by a blueprint. As we have seen, they emerge as solutions to the rules that govern the production of tissues from cells. Guided by these rules, cells find solutions that work. An emerging discipline called _synthetic morphology_ is now exploring how, and how far, those outcomes can be tailored and modified to alter the shapes and forms of living matter. The goal is not to create mermaids or other grotesque creatures, but both to better understand the rules of natural morphology and to make useful structures and devices by engineering with living tissue, with potential applications in medicine, robotics, and beyond.

While motivated by practical considerations, synthetic morphology asks deep questions that challenge conventional wisdom in biology. Where does form come from? What are the rules that evolution has developed for controlling it? And what happens when we tinker with them to exploit and extend the plasticity of living matter? The possibilities seem to be limited only by our imagination. “You could imagine developing organs that don’t exist yet,” says bioengineer Roger Kamm. For example, we might design an organ that secretes a particular biomolecule to treat a disease, rather as the pancreas secretes insulin. It could come complete with sensor cells that monitor molecular markers of the disease in the bloodstream—like the artificial controlled-release implants already used to administer drugs, but alive and integrated into the body. Or, Kamm says, we might imagine making “super-organs” that do what existing organs do, but better: eyes, say, that can register infrared or ultraviolet light.

Besides such biomedical applications, this enterprise could become part of traditional engineering. Already researchers are using living tissues as active parts in robots—a blend of organic, living materials and purely inorganic ones. Ultimately, we can imagine creating entirely new living beings shaped not by evolution but by our own designs. “By studying natural organisms, we are just exploring a tiny corner of the option space of all possible beings,” says biologist Michael Levin. “Now we have the opportunity to really explore this space.”

Genetic engineering works perfectly well for some purposes. By inserting a gene for making insulin into bacteria, for example, this compound, vital for treating diabetes, can be made by microorganisms cultured in vats, instead of having to extract it from cows and pigs. Genetically engineered bacteria are now widely used as “living factories” for making a wide range of protein-based drugs, including hormones, growth factors, enzymes, and antibodies. The possibilities of genetic engineering have been greatly enhanced by the discovery of a more accurate molecular system for editing genes, called CRISPR-Cas9. This makes use of a DNA-cutting enzyme called Cas9 that occurs naturally in bacteria, and which can be reliably programmed to find a specific target sequence in a strand of DNA. The enzyme carries a piece of RNA holding the sequence of the target site. When the enzyme finds the DNA sequence matching its RNA reference strand, it snips the DNA double helix in two. Other enzymes can then insert another piece of DNA into the break.

Artemisinin synthesis in yeast is often regarded as the poster child of a discipline called _synthetic biology_. The field has been advertised as “genetic engineering that really works”: using the same cut-and-paste biotechnological methods as the older, mature discipline, but with a sophistication that gets results beyond merely giving bacteria a new trick or two. Synthetic biologists imagine, for example, engineering bacteria or yeast that can then be cultured in vats and fed with waste plant matter to make “green” fuels, such as hydrogen or ethanol, negating the need to extract and burn coal and oil. They imagine biodegradable plastics produced by living cells rather than from oil. The language of this new science is that of the engineer and designer: that is, of the artisan rather than of the natural philosopher discovering how nature works. Synthetic biology brings a Newtonian, mechanistic philosophy to bear on the very stuff of life, the genes and enzymes of living cells. These molecular components are regarded as cogs and gears that can be filed, spring-loaded, oiled, and assembled into new mechanisms of life itself. In practice, the metaphor deployed for this kind of work is not that of clockwork and mechanics but of our latest cutting-edge technology: electronics and computation. Different components in the genome are linked into circuits and regulated by feedback loops and switches as they pass signals from one unit to another. And it works.

The notion of re-engineering life has always been controversial. In the mid-1970s, scientists involved in the emerging discipline of genetic engineering debated whether they ought to self-regulate what should and should not be permitted with this powerful new technology. Because synthetic biology increases the ambitions and the possibilities, it also raises the stakes. What if, for example, new strains of bacteria were developed with unprecedented capabilities—perhaps enhanced pathogenicity, or the ability to replicate faster than any natural species? How could they be kept under control? One way might be to build in safeguards. For instance, the innate ability of bacteria to respond to high population density by altering their replication rate (a feature known as quorum sensing) could be co-opted to activate a self-destruct mechanism. Or we might build in gene circuits that function like the logic gates of computers to count the number of times a cell divides, and flip a switch so that after a certain number of cycles the cells spontaneously die. Or we could make the organisms dependent on some substance not found in the natural environment, such as a human-made amino acid for making their proteins, so that they can’t flourish without our explicit help. Yet as synthetic biology develops, it will be hard to anticipate all the possible problems, whether malevolent or inadvertent.

Synthetic biologists like to quote the legendary physicist Richard Feynman, who wrote shortly before his death that “What I cannot create, I do not understand.” The ability to design and create a new organism—or at least an organism capable of new, “nonnatural” functions—is for them a demonstration not so much of Faustian technical mastery as of godlike knowledge. It would not be enough—and perhaps not be possible—simply to make something that works without a full understanding of why it does what it does. This, says historian of science Sophia Roosth, is truly what distinguishes synthetic biologists from old-school genetic engineers:

> The organisms conceived by this latest crop of mechanical and electrical engineers-cum-biologists ... are altogether different from the creatures built by biotechnologists: while some are made to serve discrete pharmaceutical or agricultural functions, many of them are made as a way of _theorizing the biological_.

Working out the rules governing synthetic morphology is a much harder task than merely figuring out how to build with blocks that have specific assembly rules, like Lego bricks—because with cells, those rules are themselves changed by the assembly process. “In a simple mechanical world you would have pieces that interact with each other following a set of rules to build more complex structures,” says Marta Shahbazi. But it’s the very beauty of development, she adds—as well as the reason it is so complicated—that “the process of building a structure changes the very nature of the building blocks. Throughout development there is a constant crosstalk from processes that happen at different scales of biological organization.”

Synthetic morphology therefore demands a new view of engineering, in which we assemble objects from their basic components not in a simple assembly-line manner according to a blueprint but by exploiting rules of interaction to enable a desired structure to _emerge_—as if, you might say, by cellular consent. French computational biologist René Doursat calls this “morphological engineering,” and he identifies four categories of process that it entails:

- _Constructing_: the agents attach to one another in a programmed way.

- _Coalescing_: the agents assemble via swarm-like behavior.

- _Developing_: the morphology emerges by growth and multiplication of the components.

- _Generating_: structure emerges by the repetitive unfolding of an algorithm, like that which produces the fractal forms of plants.

To these principles I would suggest adding a fifth:

- _Transforming_: the agents interact with their environment and their neighbors to develop entirely new behavioral capabilities—a process that is, in principle, open-ended.

The challenge, Doursat says, is to find ways of generating outcomes from the operation of these principles that are both robust—so that they will reliably appear in a given set of circumstances, and not be destroyed by small perturbations—and adaptive, so that when the circumstances change, the system is able to find a new solution that does the job. The philosophy has much more in common with the way we create cities and societies: we have some idea of what we’d like to see emerge, but we can’t totally control it from the bottom up. Rather, we can only try to guide the self-organization along the right lines.

If we do become adept at designing living morphology, we may need to recognize too that we could be at the same time tinkering with cognition. For as we’ve seen, cells, like people, are fundamentally cognitive agents that try to get along with whatever world they’re given. And all cognition is embodied, influenced by the physical form that contains it. Our own patterns of cognition are shaped by the kinds of bodies we have: we make interventions in the world on the basis of assumptions about what our bodies can do. If we reshape the human form, we will reshape our minds too. That six-fingered pianist in _Gattaca_ would have needed a mental representation of his extra digit in his brain in order to be able to use it effectively. This remains true even for living forms that do not have a brain and nervous system as complex as ours: _any_ repertoire of interventions an organism has is constrained by its shape and form. Reconfigured life will not just be able to do new things but will in some sense _think_ of new things to do.

“All minds emerge to find themselves in [what is] to them a novel ‘world’ and must adapt to the structure of the body and the external environment,” says Levin.

> The remarkable thing about synthetic organisms is that they enable us to observe cognition in bodies that are created de novo for the first time on Earth, with no lengthy evolutionary back-story. What kinds of minds are immediately manifested in entirely new life forms?

Through synthetic morphology, then, we may end up looking not just at “life as it _could_ be,” but at “minds as they _could_ be.” We might find ourselves rethinking thinking itself.
